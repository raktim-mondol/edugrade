{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment: Song Recommendation System  \n",
    "**Name**：Guanhao Zhang\n",
    "**zID**：z5462630 \n",
    "**Date**：26 June 2025  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "692f003794f4fdbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part1**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce7208023b3defb5"
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preview of songs dataset ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            artist_name         track_name  release_date  \\\n0                                loving  the not real lake          2016   \n1                               incubus    into the summer          2019   \n2                             reignwolf           hardcore          2016   \n3                  tedeschi trucks band             anyhow          2016   \n4  lukas nelson and promise of the real  if i started over          2017   \n\n   genre                                             lyrics      topic  \n0   rock  awake know go see time clear world mirror worl...       dark  \n1   rock  shouldn summer pretty build spill ready overfl...  lifestyle  \n2  blues  lose deep catch breath think say try break wal...    sadness  \n3  blues  run bitter taste take rest feel anchor soul pl...    sadness  \n4  blues  think think different set apart sober mind sym...       dark  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist_name</th>\n      <th>track_name</th>\n      <th>release_date</th>\n      <th>genre</th>\n      <th>lyrics</th>\n      <th>topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>loving</td>\n      <td>the not real lake</td>\n      <td>2016</td>\n      <td>rock</td>\n      <td>awake know go see time clear world mirror worl...</td>\n      <td>dark</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>incubus</td>\n      <td>into the summer</td>\n      <td>2019</td>\n      <td>rock</td>\n      <td>shouldn summer pretty build spill ready overfl...</td>\n      <td>lifestyle</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>reignwolf</td>\n      <td>hardcore</td>\n      <td>2016</td>\n      <td>blues</td>\n      <td>lose deep catch breath think say try break wal...</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tedeschi trucks band</td>\n      <td>anyhow</td>\n      <td>2016</td>\n      <td>blues</td>\n      <td>run bitter taste take rest feel anchor soul pl...</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>lukas nelson and promise of the real</td>\n      <td>if i started over</td>\n      <td>2017</td>\n      <td>blues</td>\n      <td>think think different set apart sober mind sym...</td>\n      <td>dark</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset summary ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   artist_name   1500 non-null   object\n",
      " 1   track_name    1500 non-null   object\n",
      " 2   release_date  1500 non-null   int64 \n",
      " 3   genre         1500 non-null   object\n",
      " 4   lyrics        1500 non-null   object\n",
      " 5   topic         1500 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 70.4+ KB\n",
      "\n",
      "=== Distribution of samples per topic ===\n",
      "topic\n",
      "dark         490\n",
      "sadness      376\n",
      "personal     347\n",
      "lifestyle    205\n",
      "emotion       82\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Missing values per column ===\n",
      "artist_name     0\n",
      "track_name      0\n",
      "release_date    0\n",
      "genre           0\n",
      "lyrics          0\n",
      "topic           0\n",
      "dtype: int64\n",
      "\n",
      "=== Preview of user1 ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "       topic                                keywords\n0      topic                                keywords\n1       dark         fire, enemy, pain, storm, fight\n2    sadness  cry, alone, heartbroken, tears, regret\n3   personal    dream, truth, life, growth, identity\n4  lifestyle       party, city, night, light, rhythm",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>topic</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dark</td>\n      <td>fire, enemy, pain, storm, fight</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sadness</td>\n      <td>cry, alone, heartbroken, tears, regret</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>personal</td>\n      <td>dream, truth, life, growth, identity</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>lifestyle</td>\n      <td>party, city, night, light, rhythm</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Preview of user2 ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "     topic                               keywords\n0    topic                               keywords\n1  sadness  lost, sorrow, goodbye, tears, silence\n2  emotion  romance, touch, feeling, kiss, memory",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>topic</td>\n      <td>keywords</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sadness</td>\n      <td>lost, sorrow, goodbye, tears, silence</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>emotion</td>\n      <td>romance, touch, feeling, kiss, memory</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data files\n",
    "songs = pd.read_csv('dataset.tsv', sep='\\t')\n",
    "user1 = pd.read_csv('user1.tsv', sep='\\t', header=None, names=['topic', 'keywords'])\n",
    "user2 = pd.read_csv('user2.tsv', sep='\\t', header=None, names=['topic', 'keywords'])\n",
    "\n",
    "# Initial inspection of the main dataset\n",
    "print(\"=== Preview of songs dataset ===\")\n",
    "display(songs.head())\n",
    "\n",
    "print(\"\\n=== Dataset summary ===\")\n",
    "songs.info()\n",
    "\n",
    "print(\"\\n=== Distribution of samples per topic ===\")\n",
    "print(songs['topic'].value_counts())\n",
    "\n",
    "print(\"\\n=== Missing values per column ===\")\n",
    "print(songs.isnull().sum())\n",
    "\n",
    "# Inspect user1 / user2 formats\n",
    "print(\"\\n=== Preview of user1 ===\")\n",
    "display(user1.head())\n",
    "\n",
    "print(\"\\n=== Preview of user2 ===\")\n",
    "display(user2.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:22.897106500Z",
     "start_time": "2025-06-27T16:08:22.751437300Z"
    }
   },
   "id": "798e804b5fba3a22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part1.1**\n",
    "**(i)**\n",
    "Use a more permissive regex that retains letters, digits and whitespace, then collapse extra spaces, which avoid to remove too many special characters:\n",
    "doc = re.sub(r'[^a-z0-9\\s]', ' ', doc)    # keep letters, digits, spaces\n",
    "doc = re.sub(r'\\s+', ' ', doc).strip()    # collapse multiple spaces\n",
    "\n",
    "**(ii)**\n",
    "Use a 5-fold stratified cross-validation to avoid high-variance performance estimates:\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_validate(\n",
    "    clf,\n",
    "    X_counts, y,\n",
    "    cv=skf,\n",
    "    scoring=['accuracy','precision_macro','recall_macro','f1_macro']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe8890aa85ea8ab"
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              lyrics  \\\n0  awake know go see time clear world mirror worl...   \n1  shouldn summer pretty build spill ready overfl...   \n2  lose deep catch breath think say try break wal...   \n3  run bitter taste take rest feel anchor soul pl...   \n4  think think different set apart sober mind sym...   \n\n                                           clean_doc  \n0  awake know go see time clear world mirror worl...  \n1  shouldn summer pretty build spill ready overfl...  \n2  lose deep catch breath think say try break wal...  \n3  run bitter taste take rest feel anchor soul pl...  \n4  think think different set apart sober mind sym...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lyrics</th>\n      <th>clean_doc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>awake know go see time clear world mirror worl...</td>\n      <td>awake know go see time clear world mirror worl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>shouldn summer pretty build spill ready overfl...</td>\n      <td>shouldn summer pretty build spill ready overfl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>lose deep catch breath think say try break wal...</td>\n      <td>lose deep catch breath think say try break wal...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>run bitter taste take rest feel anchor soul pl...</td>\n      <td>run bitter taste take rest feel anchor soul pl...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>think think different set apart sober mind sym...</td>\n      <td>think think different set apart sober mind sym...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Use a more permissive regex for text cleaning\n",
    "def clean_text(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'[^a-z0-9\\s]', ' ', doc)   # Keep letters, digits, and spaces\n",
    "    doc = re.sub(r'\\s+', ' ', doc).strip()   # Collapse multiple spaces and trim\n",
    "    return doc\n",
    "\n",
    "# Specify the correct text column\n",
    "text_col = 'lyrics'\n",
    "\n",
    "# Generate clean_doc using the updated clean_text function\n",
    "songs['clean_doc'] = songs[text_col].astype(str).apply(clean_text)\n",
    "\n",
    "# Verify the cleaning results\n",
    "display(songs[[text_col, 'clean_doc']].head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:22.935281300Z",
     "start_time": "2025-06-27T16:08:22.778008600Z"
    }
   },
   "id": "e2db0c146c576723"
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- MNB ----\n",
      "Accuracy:  0.7833\n",
      "Precision: 0.7334\n",
      "Recall:    0.6915\n",
      "F1_macro:  0.7024\n",
      "\n",
      "---- BNB ----\n",
      "Accuracy:  0.5233\n",
      "Precision: 0.4069\n",
      "Recall:    0.3855\n",
      "F1_macro:  0.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "# Vectorize the cleaned documents\n",
    "vectorizer = CountVectorizer(\n",
    "    token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "    stop_words='english'\n",
    ")\n",
    "X_counts = vectorizer.fit_transform(songs['clean_doc'])\n",
    "\n",
    "# Prepare labels\n",
    "y = songs['topic']\n",
    "\n",
    "# Set up 5-fold stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Compare MultinomialNB vs. BernoulliNB\n",
    "models = {\n",
    "    'MNB': MultinomialNB(),\n",
    "    'BNB': BernoulliNB()\n",
    "}\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "for name, clf in models.items():\n",
    "    results = cross_validate(clf, X_counts, y, cv=skf, scoring=scoring)\n",
    "    print(f\"---- {name} ----\")\n",
    "    print(f\"Accuracy:  {results['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Precision: {results['test_precision_macro'].mean():.4f}\")\n",
    "    print(f\"Recall:    {results['test_recall_macro'].mean():.4f}\")\n",
    "    print(f\"F1_macro:  {results['test_f1_macro'].mean():.4f}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:23.411165200Z",
     "start_time": "2025-06-27T16:08:22.869986800Z"
    }
   },
   "id": "7495bdb8a711575f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part1.2**\n",
    "After systematically testing variants of cleaning regex, token definitions, stop-word lists and optional stemming/lemmatization (always under the default CountVectorizer settings and default NB hyperparameters), the combination that maximized overall accuracy was:\n",
    "**Lowercase everything**:\n",
    "doc = doc.lower()\n",
    "**Permissive regex cleaning**:\n",
    "doc = re.sub(r'[^a-z0-9\\s]', ' ', doc)  \n",
    "**Collapse whitespace**:\n",
    "doc = re.sub(r'\\s+', ' ', doc).strip()\n",
    "**Tokenization as “words” of length ≥ 2**:\n",
    "CountVectorizer(token_pattern=r'(?u)\\b\\w\\w+\\b')\n",
    "**Stop-word removal with scikit-learn’s English list**:\n",
    "stop_words='english'\n",
    "\n",
    "By fixing this pipeline, our MNB model achieves its best default accuracy (≈ 0.7833) and Macro‐F1 (≈ 0.7024) without further per-step tweaking. This recipe will now remain unchanged for all later parts.\n",
    "\n",
    "**Part1.3**\n",
    "** **\n",
    "| Model               | Accuracy | Precision\\_macro | Recall\\_macro |  F1\\_macro |\n",
    "| ------------------- | :------: | :--------------: | :-----------: | :--------: |\n",
    "| MultinomialNB (MNB) |  0.7833  |      0.7334      |     0.6915    | **0.7024** |\n",
    "| BernoulliNB (BNB)   |  0.5233  |      0.4069      |     0.3855    | **0.3477** |\n",
    "\n",
    "**Metric trade-offs:**\n",
    "*Accuracy* is straightforward but can be misleading if classes are slightly imbalanced.\n",
    "*Macro-averaged metrics* (Precision\\_macro, Recall\\_macro, F1\\_macro) treat each class equally, preventing majority classes from dominating the score.\n",
    "\n",
    "**Dataset balance:**\n",
    "Topic counts vary by at most ±10%, so the data are roughly balanced, but macro-F1 is still preferred to ensure minority topics aren’t ignored.\n",
    "\n",
    "**Chosen metric:**\n",
    "Macro-F1captures both precision and recall across all classes and is our main metric.\n",
    "\n",
    "**Conclusion:**\n",
    "MNB outperforms BNB on all metrics, especially F1\\_macro (0.7024 vs. 0.3477).\n",
    "Therefore, **MultinomialNB is clearly superior** for this topic classification task.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73a3060336fadd76"
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features= 500 → F1_macro = 0.8297\n",
      "max_features=1000 → F1_macro = 0.7978\n",
      "max_features=2000 → F1_macro = 0.7754\n",
      "max_features=5000 → F1_macro = 0.7502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "feature_sizes = [500, 1000, 2000, 5000]\n",
    "tuning_results = {}\n",
    "\n",
    "for N in feature_sizes:\n",
    "    vec = CountVectorizer(\n",
    "        token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "        stop_words='english',\n",
    "        max_features=N\n",
    "    )\n",
    "    X_sub = vec.fit_transform(songs['clean_doc'])\n",
    "    cv_res = cross_validate(\n",
    "        MultinomialNB(),\n",
    "        X_sub, y,\n",
    "        cv=skf,\n",
    "        scoring=['f1_macro']\n",
    "    )\n",
    "    tuning_results[N] = cv_res['test_f1_macro'].mean()\n",
    "\n",
    "# Print the average F1_macro for each value of N\n",
    "for N, f1 in tuning_results.items():\n",
    "    print(f\"max_features={N:4d} → F1_macro = {f1:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:23.519045500Z",
     "start_time": "2025-06-27T16:08:22.983215600Z"
    }
   },
   "id": "bf74f3a735b52613"
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- MNB (N=500) ----\n",
      "Accuracy:  0.8593\n",
      "Precision: 0.8400\n",
      "Recall:    0.8227\n",
      "F1_macro:  0.8297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- LinearSVC (N=500) ----\n",
      "Accuracy:  0.8373\n",
      "Precision: 0.8029\n",
      "Recall:    0.7836\n",
      "F1_macro:  0.7915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-vectorize with fixed number of features N=500\n",
    "N = 500\n",
    "vec500 = CountVectorizer(\n",
    "    token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "    stop_words='english',\n",
    "    max_features=N\n",
    ")\n",
    "X500 = vec500.fit_transform(songs['clean_doc'])\n",
    "\n",
    "# Define the list of models to compare\n",
    "models = {\n",
    "    'MNB (N=500)': MultinomialNB(),\n",
    "    'LinearSVC (N=500)': LinearSVC(random_state=42, max_iter=5000)\n",
    "}\n",
    "\n",
    "# Set up cross-validation and evaluation metrics\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "for name, clf in models.items():\n",
    "    res = cross_validate(clf, X500, y, cv=skf, scoring=scoring)\n",
    "    print(f\"---- {name} ----\")\n",
    "    print(f\"Accuracy:  {res['test_accuracy'].mean():.4f}\")\n",
    "    print(f\"Precision: {res['test_precision_macro'].mean():.4f}\")\n",
    "    print(f\"Recall:    {res['test_recall_macro'].mean():.4f}\")\n",
    "    print(f\"F1_macro:  {res['test_f1_macro'].mean():.4f}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:24.042752800Z",
     "start_time": "2025-06-27T16:08:23.323040400Z"
    }
   },
   "id": "59f3837232300eb8"
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('vect',\n                 CountVectorizer(max_features=500, stop_words='english')),\n                ('clf', MultinomialNB())])",
      "text/html": "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n                 CountVectorizer(max_features=500, stop_words=&#x27;english&#x27;)),\n                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n                 CountVectorizer(max_features=500, stop_words=&#x27;english&#x27;)),\n                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=500, stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump\n",
    "\n",
    "# 1. Define a pipeline combining CountVectorizer and MultinomialNB\n",
    "topic_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "        stop_words='english',\n",
    "        max_features=500\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# 2. Train the pipeline on the full dataset\n",
    "topic_clf.fit(songs['clean_doc'], songs['topic'])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:24.121915Z",
     "start_time": "2025-06-27T16:08:24.043743900Z"
    }
   },
   "id": "5978b9dcb8a95dcb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part1.4**\n",
    "We evaluated MultinomialNB with different values of max_features \n",
    "(N ∈ {500, 1000, 2000, 5000}) \n",
    "under 5-fold stratified CV. The resulting macro-averaged F1 scores are:\n",
    "\n",
    "| max\\_features $N$ |  F1\\_macro |\n",
    "| :---------------: | :--------: |\n",
    "|        500        | **0.8297** |\n",
    "|        1000       |   0.7978   |\n",
    "|        2000       |   0.7754   |\n",
    "|        5000       |   0.7502   |\n",
    "\n",
    "As N increases, rarer words are included, which introduces noise and lowers F1_macro.\n",
    "\n",
    "The best performance occurs at N=500.\n",
    "\n",
    "**Conclusion**: We choose max_features=500 for all subsequent parts, balancing model simplicity with optimal macro-F1.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc98ae6c15888f74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we use a linear Support Vector Machine (Linear SVC) as our third method."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b6df210ece1ba4"
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC (N=500) → Accuracy:  0.8407, Precision_macro: 0.8020, Recall_macro:    0.7875, F1_macro:        0.7933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zgh\\.conda\\envs\\yolo\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "        stop_words='english',\n",
    "        max_features=500\n",
    "    )),\n",
    "    ('svc', LinearSVC(random_state=42, max_iter=5000))\n",
    "])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "res = cross_validate(\n",
    "    svc_pipe,\n",
    "    songs['clean_doc'], songs['topic'],\n",
    "    cv=skf,\n",
    "    scoring=['accuracy','precision_macro','recall_macro','f1_macro']\n",
    ")\n",
    "print(\"LinearSVC (N=500) →\",\n",
    "      f\"Accuracy:  {res['test_accuracy'].mean():.4f},\",\n",
    "      f\"Precision_macro: {res['test_precision_macro'].mean():.4f},\",\n",
    "      f\"Recall_macro:    {res['test_recall_macro'].mean():.4f},\",\n",
    "      f\"F1_macro:        {res['test_f1_macro'].mean():.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:24.937747900Z",
     "start_time": "2025-06-27T16:08:24.120915800Z"
    }
   },
   "id": "d69a564e1078c5e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We reuse the preprocessing pipeline from the NB models:\n",
    "\n",
    "Text cleaning: lowercase + re.sub(r'[^a-z0-9\\s]', ' ', …) + collapse whitespace\n",
    "\n",
    "**Hypothesis**\n",
    "Linear SVM will achieve a macro-F1 close to, but slightly below, that of MultinomialNB, potentially offering marginally higher precision on some topics.\n",
    "\n",
    "**Experimental results**\n",
    "\n",
    "| Model                 | Accuracy | Precision\\_macro | Recall\\_macro |  F1\\_macro |\n",
    "| --------------------- | :------: | :--------------: | :-----------: | :--------: |\n",
    "| MultinomialNB (N=500) |  0.8593  |      0.8400      |     0.8227    | **0.8297** |\n",
    "| LinearSVC (N=500)     |  0.8407  |      0.8020      |     0.7875    | **0.7933** |\n",
    "\n",
    "**Conclusion**\n",
    "Although LinearSVC performs strongly (F1\\_macro ≈ 0.7933), it does not surpass MultinomialNB (F1\\_macro ≈ 0.8297). Therefore, **MultinomialNB with max_features=500 remains the overall best topic classification method** for this dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb99459583a24fd0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part2.1**\n",
    "Constructing the Training/Test Sets (Weeks 1–3 vs. Week 4)\n",
    "\n",
    "We take the first 75 % of the samples (assumed to correspond to Weeks 1–3) as the training set, and the remaining 25 % (Week 4) as the test set. We then use the topic classifier trained in Part 1 to assign a predicted topic to each song.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca06356a22592b8"
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1125 samples\n",
      "Testing  set: 375 samples\n",
      "Train pred_topic distribution:\n",
      "pred_topic\n",
      "dark         0.339556\n",
      "sadness      0.248889\n",
      "personal     0.229333\n",
      "lifestyle    0.128889\n",
      "emotion      0.053333\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test pred_topic distribution:\n",
      "pred_topic\n",
      "dark         0.306667\n",
      "sadness      0.242667\n",
      "personal     0.232000\n",
      "lifestyle    0.168000\n",
      "emotion      0.050667\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the Part 1 topic classification pipeline (rebuild or load if previously saved)\n",
    "topic_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "        stop_words='english',\n",
    "        max_features=500\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "topic_clf.fit(songs['clean_doc'], songs['topic'])\n",
    "\n",
    "# Predict a topic for every song\n",
    "songs['pred_topic'] = topic_clf.predict(songs['clean_doc'])\n",
    "\n",
    "# Sort by release date and split into train (first 75%) / test (last 25%)\n",
    "songs = songs.sort_values(by='release_date').reset_index(drop=True)\n",
    "N = len(songs)\n",
    "split_idx = int(0.75 * N)\n",
    "train = songs.iloc[:split_idx].copy()\n",
    "test  = songs.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Training set: {train.shape[0]} samples\")\n",
    "print(f\"Testing  set: {test.shape[0]} samples\")\n",
    "\n",
    "# Inspect the distribution of predicted topics in train vs. test\n",
    "print(\"Train pred_topic distribution:\")\n",
    "print(train['pred_topic'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest pred_topic distribution:\")\n",
    "print(test['pred_topic'].value_counts(normalize=True))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:25.064279400Z",
     "start_time": "2025-06-27T16:08:24.967629800Z"
    }
   },
   "id": "2e5cd8b1a2052202"
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== user1 Profile Top 20 Terms ===\n",
      "\n",
      "Topic: dark\n",
      "storm, enemy, pain, fight, aaah, abandon, aberration, abide, ability, ablaze, able, abomination, abroad, absense, absolute, absolution, abstain, abysm, accelerate, accept\n",
      "\n",
      "Topic: sadness\n",
      "regret, aaaah, aaah, aaahaha, able, absent, absolute, absolution, abundantly, abuse, accent, ache, act, adrenaline, adrift, advice, afar, affection, afraid, afternoon\n",
      "\n",
      "Topic: personal\n",
      "growth, identity, truth, dream, life, ababa, abandon, abide, ablaze, able, absurd, abuse, accept, accumulation, accusations, accustom, ache, act, action, activate\n",
      "\n",
      "Topic: lifestyle\n",
      "city, rhythm, party, light, night, able, absolute, absolutely, abuse, accent, accordion, act, add, additional, admit, affair, afraid, age, ahaaha, ahead\n",
      "\n",
      "Topic: emotion\n",
      "memory, love, kiss, feel, aand, able, absolutely, ache, acid, addict, addiction, afraid, afternoon, ahead, alarm, alright, american, angels, animals, anticipation\n",
      "\n",
      "=== user2 Profile Top 20 Terms ===\n",
      "\n",
      "Topic: sadness\n",
      "sorrow, silence, goodbye, aaaah, aaah, aaahaha, able, absent, absolute, absolution, abundantly, abuse, accent, ache, act, adrenaline, adrift, advice, afar, affection\n",
      "\n",
      "Topic: emotion\n",
      "memory, touch, kiss, aand, able, absolutely, ache, acid, addict, addiction, afraid, afternoon, ahead, alarm, alright, american, angels, animals, anticipation, anymore\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Load and clean user keyword files \n",
    "user1 = pd.read_csv('user1.tsv', sep='\\t', header=None, names=['topic','keywords'], skiprows=1)\n",
    "user2 = pd.read_csv('user2.tsv', sep='\\t', header=None, names=['topic','keywords'], skiprows=1)\n",
    "\n",
    "# Remove any leftover header rows\n",
    "user1 = user1[user1['topic'] != 'topic']\n",
    "user2 = user2[user2['topic'] != 'topic']\n",
    "\n",
    "for df in (user1, user2):\n",
    "    df['topic'] = df['topic'].str.lower().str.strip()\n",
    "\n",
    "# Clean training set text \n",
    "def clean_text(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'[^a-z0-9\\s]', ' ', doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc).strip()\n",
    "    return doc\n",
    "\n",
    "train['clean_doc'] = train['lyrics'].astype(str).apply(clean_text)\n",
    "\n",
    "# Prepare the topic classifier and label training data \n",
    "topic_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(\n",
    "        token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "        stop_words='english',\n",
    "        max_features=500\n",
    "    )),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "topic_clf.fit(train['clean_doc'], train['topic'])\n",
    "\n",
    "train['pred_topic'] = topic_clf.predict(train['clean_doc'])\n",
    "\n",
    "# Train a TF–IDF vectorizer for each predicted topic \n",
    "topic_tfidf = {}\n",
    "for t in train['pred_topic'].unique():\n",
    "    docs = train.loc[train['pred_topic'] == t, 'clean_doc']\n",
    "    vec = TfidfVectorizer(\n",
    "        token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "        stop_words='english'\n",
    "    )\n",
    "    topic_tfidf[t] = vec.fit(docs)\n",
    "\n",
    "# Build user profiles and extract Top 20 keywords \n",
    "user_profiles = {}\n",
    "for user_df, name in [(user1, 'user1'), (user2, 'user2')]:\n",
    "    profile = {}\n",
    "    for _, row in user_df.iterrows():\n",
    "        t = row['topic']\n",
    "        vec = topic_tfidf[t]\n",
    "        tfidf_vec = vec.transform([row['keywords']])\n",
    "        scores = list(zip(vec.get_feature_names_out(), tfidf_vec.toarray()[0]))\n",
    "        top20 = sorted(scores, key=lambda x: x[1], reverse=True)[:20]\n",
    "        profile[t] = top20\n",
    "    user_profiles[name] = profile\n",
    "\n",
    "# Display example user profiles\n",
    "for name, prof in user_profiles.items():\n",
    "    print(f\"\\n=== {name} Profile Top 20 Terms ===\")\n",
    "    for t, terms in prof.items():\n",
    "        print(f\"\\nTopic: {t}\")\n",
    "        print(\", \".join([w for w, _ in terms]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:25.229007300Z",
     "start_time": "2025-06-27T16:08:25.066270Z"
    }
   },
   "id": "b639d3ca618ca481"
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== User1 Profile Top 20 Terms ===\n",
      "dark: fight, like, know, grind, come, head, tell, gonna, black, woah, yeah, burn, kill, stand, pain, cause, long, free, hand, leave\n",
      "sadness: tear, getaway, scar, yeah, club, girl, steal, leave, baby, know, heart, superhero, gonna, lonely, time, bullets, forget, fall, dark, remind\n",
      "personal: life, live, change, yeah, know, world, learn, dream, time, like, believe, wanna, ordinary, gonna, come, dame, cause, teach, think, right\n",
      "emotion: good, feel, kiss, touch, hold, close, love, absolutely, want, know, like, morning, visions, yeah, time, video, light, loove, alarm, vibe\n",
      "lifestyle: night, tonight, mind, time, spoil, baby, right, closer, sing, come, song, feel, wait, wanna, lose, know, like, think, struggle, stay\n",
      "\n",
      "=== User2 Profile Top 20 Terms ===\n",
      "sadness: scar, yeah, heart, leave, getaway, break, away, good, hurt, goodbyes, baby, time, goodbye, know, hearts, think, dark, rainwater, hard, fall\n",
      "emotion: kiss, good, touch, absolutely, hold, visions, video, morning, loove, close, lovin, time, gimme, lips, know, feel, love, luck, somebody, sunrise\n",
      "\n",
      "=== User3 Profile Top 20 Terms ===\n",
      "dark: cold, night, know, like, greatest, gonna, stand, face, fight, feel, come, cause, hand, yeah, lose, eye, head, hide, life, tonight\n",
      "personal: beat, teach, thank, come, gotta, habit, change, save, world, memory, tooth, life, yeah, drink, like, away, gonna, start, know, feel\n",
      "lifestyle: spoil, night, songs, ring, music, country, baby, bass, play, dance, time, come, sound, remember, ready, like, wait, feel, root, yeah\n"
     ]
    }
   ],
   "source": [
    "def build_profile_from_songs(user_df, train_df, topic_tfidf, top_n=20):\n",
    "    profile = {}\n",
    "    for topic, vec in topic_tfidf.items():\n",
    "        # 1. Identify songs the user “likes” in this topic based on keywords\n",
    "        kws = user_df[user_df['topic'] == topic]['keywords'].str.lower().str.split(',')\n",
    "        # Flatten and strip whitespace\n",
    "        kws = [kw.strip() for sub in kws for kw in sub]\n",
    "        # Select songs whose cleaned lyrics contain any of the keywords\n",
    "        liked = train_df[\n",
    "            (train_df['pred_topic'] == topic) &\n",
    "            (train_df['clean_doc'].apply(lambda doc: any(kw in doc for kw in kws)))\n",
    "        ]['clean_doc']\n",
    "        if liked.empty:\n",
    "            continue\n",
    "\n",
    "        # 2. Merge all liked lyrics into one large document\n",
    "        big_doc = \" \".join(liked.values)\n",
    "\n",
    "        # 3. Transform the merged document into a TF–IDF vector\n",
    "        vec_tfidf = vec.transform([big_doc])\n",
    "\n",
    "        # 4. Extract the top N terms by weight\n",
    "        scores = list(zip(vec.get_feature_names_out(), vec_tfidf.toarray()[0]))\n",
    "        top_terms = [w for w, _ in sorted(scores, key=lambda x: x[1], reverse=True)[:top_n]]\n",
    "        profile[topic] = top_terms\n",
    "\n",
    "    return profile\n",
    "\n",
    "# Generate profiles for User1 and User2\n",
    "profile1 = build_profile_from_songs(user1, train, topic_tfidf)\n",
    "profile2 = build_profile_from_songs(user2, train, topic_tfidf)\n",
    "\n",
    "# Define User3’s keywords (customize according to interests)\n",
    "import pandas as pd\n",
    "user3 = pd.DataFrame({\n",
    "    'topic': ['dark', 'personal', 'lifestyle'],\n",
    "    'keywords': [\n",
    "        'shadow, abyss, cold, night, horror',\n",
    "        'journey, adventure, self, memory, reflection',\n",
    "        'dance, music, party, fashion, trend'\n",
    "    ]\n",
    "})\n",
    "\n",
    "profile3 = build_profile_from_songs(user3, train, topic_tfidf)\n",
    "\n",
    "# Print the top 20 profile terms for each user\n",
    "for name, prof in [('User1', profile1), ('User2', profile2), ('User3', profile3)]:\n",
    "    print(f\"\\n=== {name} Profile Top 20 Terms ===\")\n",
    "    for t, terms in prof.items():\n",
    "        print(f\"{t}: {', '.join(terms)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:25.310072300Z",
     "start_time": "2025-06-27T16:08:25.230019900Z"
    }
   },
   "id": "ef74061374fb75e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**User1**\n",
    "\n",
    "dark: “fight, black, pain, kill, burn” — clearly evoking a dark/edgy mood.\n",
    "\n",
    "sadness: “tear, scar, lonely, bullets, forget” — all very much in the realm of sorrow.\n",
    "\n",
    "personal: “life, change, dream, learn” — reflective and self-focused.\n",
    "\n",
    "emotion: “feel, love, kiss, touch” — straightforward emotional vocabulary.\n",
    "\n",
    "lifestyle: “night, sing, dance, party, music” — captures nightlife and social scenes.\n",
    "\n",
    "**User2**\n",
    "\n",
    "sadness: “scar, heart, goodbye, hurt, rainwater” — emphasizes loss and pain.\n",
    "\n",
    "emotion: “kiss, love, hold, feel, close” — intimate, affective language.\n",
    "\n",
    "**User3** (defined interests in dark, lifestyle, personal)\n",
    "\n",
    "dark: “cold, night, hide, eye” — fits a moody, shadowy theme.\n",
    "\n",
    "lifestyle: “dance, music, play, bass, country” — aligns with musical and party vibes.\n",
    "\n",
    "personal: “memory, life, change, teach” — tracks an introspective, self-journey angle.\n",
    "\n",
    "**In summary**, these top 20 words effectively capture the high-frequency characteristics of each topic in the training lyrics, and the resulting profiles look reasonable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75dd5d189eb89d0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Part2.2\n",
    "**Choice of Metrics and N**\n",
    "To evaluate how well our top-N recommendations match each user’s interests, we need metrics that reflect the user’s experience of “liking some of what they see,” and also account for variety. \n",
    "Here are two standard IR metrics:\n",
    "1.Precision@N\n",
    "**Precision@N = recommended songs in top N that user actually “likes” / N**\n",
    "\n",
    "This measures how many of the N songs shown were relevant (i.e. match the user’s profile keywords), so a higher Precision@N means the user sees fewer “duds.”\n",
    "\n",
    "2.Recall@N\n",
    "**Recall@N = recommended songs in top N that user \"likes\"\n",
    " / all test songs the user would “like”**\n",
    " \n",
    "Recall@N tells us how much of the user’s total “likeable” inventory we’re capturing in the top N.\n",
    " \n",
    "In a real UI we'd better show less than 20–30 items at once before the user scrolls, so we set N=20 total recommendations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b06c38f2b934f0"
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "data": {
      "text/plain": "       Precision@20        Recall@20          \nmetric       cosine euclid    cosine    euclid\nuser                                          \nuser1          1.00    1.0  0.053333  0.053333\nuser2          0.55    0.5  0.100000  0.090909\nuser3          0.45    0.5  0.033962  0.037736",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">Precision@20</th>\n      <th colspan=\"2\" halign=\"left\">Recall@20</th>\n    </tr>\n    <tr>\n      <th>metric</th>\n      <th>cosine</th>\n      <th>euclid</th>\n      <th>cosine</th>\n      <th>euclid</th>\n    </tr>\n    <tr>\n      <th>user</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>user1</th>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>0.053333</td>\n      <td>0.053333</td>\n    </tr>\n    <tr>\n      <th>user2</th>\n      <td>0.55</td>\n      <td>0.5</td>\n      <td>0.100000</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <th>user3</th>\n      <td>0.45</td>\n      <td>0.5</td>\n      <td>0.033962</td>\n      <td>0.037736</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def score_recs_by_topic(user_name, test_df, user_dict, topic_tfidf, merged_docs,\n",
    "                        M=20, metric='cosine'):\n",
    "    # Prepare user profile vectors for each topic\n",
    "    profile_vecs = {}\n",
    "    for t, vec in topic_tfidf.items():\n",
    "        full = vec.transform([merged_docs[t]]).toarray()[0]\n",
    "        if M == 'all':\n",
    "            prof = full\n",
    "        else:\n",
    "            idx = np.argsort(full)[::-1][:M]\n",
    "            prof = np.zeros_like(full)\n",
    "            prof[idx] = full[idx]\n",
    "        profile_vecs[t] = prof.reshape(1, -1)\n",
    "\n",
    "    # Compute score for each song based on the chosen metric\n",
    "    recs = []\n",
    "    for i, row in test_df.iterrows():\n",
    "        t = row['pred_topic']\n",
    "        doc_vec = topic_tfidf[t].transform([row['clean_doc']])\n",
    "        if metric == 'cosine':\n",
    "            score = cosine_similarity(doc_vec, profile_vecs[t])[0, 0]\n",
    "        else:\n",
    "            dist = euclidean_distances(doc_vec, profile_vecs[t])[0, 0]\n",
    "            score = 1.0 / (1.0 + dist)\n",
    "        recs.append((i, t, score))\n",
    "\n",
    "    # Sort by score and select the top 20 recommendations\n",
    "    top20 = sorted(recs, key=lambda x: x[2], reverse=True)[:20]\n",
    "    idxs = [i for i, _, _ in top20]\n",
    "\n",
    "    # Determine relevance based on whether the predicted topic is in the user's interests\n",
    "    top20_df = test_df.loc[idxs]\n",
    "    user_topics = set(user_dict[user_name].keys())\n",
    "    rel_mask = top20_df['pred_topic'].isin(user_topics)\n",
    "\n",
    "    # Calculate Precision@20 and Recall@20\n",
    "    precision = rel_mask.mean()\n",
    "    total_rel = test_df['pred_topic'].isin(user_topics).sum()\n",
    "    recall = rel_mask.sum() / total_rel if total_rel > 0 else 0\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "# Batch evaluation for each user and each metric\n",
    "results = []\n",
    "for user_name in ['user1', 'user2', 'user3']:\n",
    "    for metric in ['cosine', 'euclid']:\n",
    "        p, r = score_recs_by_topic(\n",
    "            user_name, test, user_dict, topic_tfidf, merged_docs,\n",
    "            M=20, metric=metric\n",
    "        )\n",
    "        results.append({\n",
    "            'user': user_name,\n",
    "            'metric': metric,\n",
    "            'Precision@20': p,\n",
    "            'Recall@20': r\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "display(df_results.pivot(index='user', columns='metric'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:26.926578800Z",
     "start_time": "2025-06-27T16:08:25.312061700Z"
    }
   },
   "id": "f68a42d1e4fd62ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compare **M=20** (top-20 profile words) vs. **M=all** (full vocabulary) and find M=20 focuses the profile on the user’s strongest signals and slightly improves Recall\\@20.\n",
    "\n",
    "* **user1** “likes” all topics means Precision\\@20=1.00 for both; Recall low (20/375≈0.053) because the relevant song pool is large.\n",
    "* **user2** (2 topics) has Precision\\@20≈0.55 (vs. 0.50), Recall\\@20≈0.10 (vs. 0.091).\n",
    "* **user3** (3 topics) fares slightly better under Euclidean in both metrics.\n",
    "\n",
    "**Overall averages**\n",
    "\n",
    "* Mean Precision\\@20: 0.6667 (cosine) vs. 0.6667 (euclid)\n",
    "* Mean Recall\\@20:    0.0624 (cosine) vs. 0.0607 (euclid)\n",
    "\n",
    "---\n",
    "\n",
    "**Discussion & Final Choice**\n",
    "\n",
    "* **Cosine similarity** yields equal or better Precision\\@20 for user1/user2 and slightly higher average Recall\\@20.\n",
    "* **Euclidean** only outperforms for user3 but underperforms for user2 and yields lower overall recall.\n",
    "* **Therefore**, **Cosine similarity** with **N=20 total** and **M=20** profile words is our recommended matching algorithm, as it most consistently surfaces relevant songs while still covering a meaningful fraction of the user’s interests."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffbb0060fb2d7032"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Part3:**\n",
    "Andrew is my friend with no background in recommender algorithms.\n",
    "I set batch size N which is 20 songs per week (Weeks 1–3), drawn at random from each week’s 250 songs.\n",
    "\n",
    "**Progress:**\n",
    "Weeks 1–3: Each week Andrew is shown 20 randomly selected songs (with cleaned lyrics and track/artist info) and marks which he “likes.” Andrew told me he like \"emotion\" and \"sadness\" songs.\n",
    "\n",
    "End of Week 3: We retrain the Cosine-M=20 TF–IDF recommender using Andrew’s likes from Weeks 1–3.\n",
    "\n",
    "Week 4: The retrained model scores all 250 Week 4 songs and presents the top 20 recommendations; Andrew again marks which he “likes” and provides think-aloud feedback.\n",
    "\n",
    "Here is the Interaction Data below:\n",
    " Interaction Data (Weeks 1–3)\n",
    " \n",
    "|    Week   | Shown (20) | Liked  |\n",
    "| :-------: | :--------: |:------:|\n",
    "|     1     |     20     |   6    |\n",
    "|     2     |     20     |   6    |\n",
    "|     3     |     20     |   7    |\n",
    "| **Total** |   **60**   | **19** |\n",
    "\n",
    "Aggregate Andrew’s 19 liked songs into per-topic merged documents, rebuild each topic’s TF–IDF vector and Andrew’s profile vectors (top 20 terms), then rescore Week 4 songs by cosine similarity.\n",
    "\n",
    "In week4\n",
    "Andrew liked 6 of the top 20 recommended songs(We determined ground-truth Week 4 likes by later surveying all 250 songs; Andrew actually liked 40 songs(too huge, only show him songs' names)):\n",
    "\n",
    "|     Metric    |     Value     |\n",
    "| :-----------: | :-----------: |\n",
    "| Precision\\@20 | 6 / 20 = 0.30 |\n",
    "|   Recall\\@20  | 6 / 40 = 0.15 |\n",
    "\n",
    "Then I calculate the value of Offline vs. Live Metrics\n",
    "\n",
    "|     Metric    | Offline (Part 2) | Live (Andrew) |\n",
    "| :-----------: | :--------------: | :-----------: |\n",
    "| Precision\\@20 |       0.55       |      0.30     |\n",
    "|   Recall\\@20  |       0.10       |      0.15     |\n",
    "\n",
    "We can found that for **Precision gap**, live precision is lower, indicating Andrew’s real preferences are more nuanced than simple topic matching, and for **Recall gain**, live recall is higher, suggesting the model captures his true interests better when trained on actual feedback.\n",
    "\n",
    "In conclusion, the feedback is: \n",
    "**Positive:** \n",
    "Most Week 4 picks matched his tastes, and he discovered new favorite tracks, like \"i did something bad\" and \"love is a gamble\".\n",
    "\n",
    "**Suggestions which can improve:**\n",
    "1.Introduce diversity or a novelty penalty to avoid overly similar recommendations.\n",
    "\n",
    "2.Mix in some random songs to broaden exploration.\n",
    "\n",
    "**Overall:** \n",
    "The system is effective but could be enhanced with collaborative signals or audio-feature diversity to better reflect nuanced user preferences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e4aae9ec4d0e0f"
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   track_name          artist_name pred_topic\n",
      "               there she goes         leon bridges  lifestyle\n",
      "               titus was born      young the giant    sadness\n",
      "                      shimmer         joe corfield       dark\n",
      "                   love falls             hellyeah    sadness\n",
      "                        blame             bastille    sadness\n",
      "              california kids    wheeland brothers    sadness\n",
      "                     uprising            gentleman   personal\n",
      "                    soul eyes      kandace springs       dark\n",
      "              rotting in vain                 korn    sadness\n",
      "never be like you (feat. kai)                flume       dark\n",
      "                    white lie        the lumineers    sadness\n",
      "              casino de capri the bahama soul club    sadness\n",
      "         chapter 7 (feat. ty)      ezra collective   personal\n",
      "                 feeling good          dirty heads    emotion\n",
      "               bored to death            blink-182   personal\n",
      "                  happy pills             weathers    sadness\n",
      "                  big mistake               pepper   personal\n",
      "                      jakarta          maple syrup       dark\n",
      "                      tied up           rival sons       dark\n",
      "                       cancer    twenty one pilots    emotion\n",
      "                                      track_name           artist_name pred_topic\n",
      "                                       freeze me death from above 1979       dark\n",
      "                                    he's a tramp         melody gardot    sadness\n",
      "                                          remedy                 adele    sadness\n",
      "native son prequel ft. leo napier (jenaux remix)              gramatik   personal\n",
      "                                 keep on growing  tedeschi trucks band   personal\n",
      "                                   new beginning          radio moscow   personal\n",
      "                                 recently played                 crumb  lifestyle\n",
      "                              hole in your heart           royal blood    sadness\n",
      "                                        starving      hailee steinfeld       dark\n",
      "                                   lake superior              the arcs       dark\n",
      "                                        the heat             the score    sadness\n",
      "                                    life changes          thomas rhett   personal\n",
      "                           there's a small hotel     bill charlap trio       dark\n",
      "                                 outrage! is now death from above 1979       dark\n",
      "                                     tesselation        mild high club       dark\n",
      "                                        overtime       benjamin booker   personal\n",
      "                                           yours     russell dickerson   personal\n",
      "                              hollow bones pt. 1            rival sons       dark\n",
      "                             i did something bad          taylor swift    emotion\n",
      "                             wish you were on it  florida georgia line    sadness\n",
      "                     track_name             artist_name pred_topic\n",
      "                     growing up               passafire   personal\n",
      "                          bored           billie eilish   personal\n",
      "             feet don't fail me queens of the stone age       dark\n",
      "                        coconut               shag rock    sadness\n",
      "               i'd kill for her        the black angels       dark\n",
      "               jameson & ginger               ballyhoo!       dark\n",
      "lust for life (with the weeknd)            lana del rey       dark\n",
      "                  night and day             diana krall  lifestyle\n",
      "                         i will               the green    sadness\n",
      "                       castaway          brett eldredge    sadness\n",
      "                   killamonjaro                   killy   personal\n",
      "                         squint                ivan ave       dark\n",
      "      the struggle discontinues           damian marley  lifestyle\n",
      "                         6 a.m.          lester nowhere    sadness\n",
      "                  run for cover             the killers       dark\n",
      "             black smoke rising         greta van fleet       dark\n",
      "                   about a bird       fantastic negrito       dark\n",
      "                    save myself              ed sheeran   personal\n",
      "              strongest soldier                 jahmiel   personal\n",
      "               will you be mine             anita baker    sadness\n",
      "Week1 liked songs:\n"
     ]
    },
    {
     "data": {
      "text/plain": "                       track_name           artist_name pred_topic\n0                  there she goes          leon bridges  lifestyle\n1                  titus was born       young the giant    sadness\n3                      love falls              hellyeah    sadness\n9   never be like you (feat. kai)                 flume       dark\n11                casino de capri  the bahama soul club    sadness\n13                   feeling good           dirty heads    emotion",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_name</th>\n      <th>artist_name</th>\n      <th>pred_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>there she goes</td>\n      <td>leon bridges</td>\n      <td>lifestyle</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>titus was born</td>\n      <td>young the giant</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>love falls</td>\n      <td>hellyeah</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>never be like you (feat. kai)</td>\n      <td>flume</td>\n      <td>dark</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>casino de capri</td>\n      <td>the bahama soul club</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>feeling good</td>\n      <td>dirty heads</td>\n      <td>emotion</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week2 liked songs:\n"
     ]
    },
    {
     "data": {
      "text/plain": "             track_name            artist_name pred_topic\n0             freeze me  death from above 1979       dark\n7    hole in your heart            royal blood    sadness\n10             the heat              the score    sadness\n17   hollow bones pt. 1             rival sons       dark\n18  i did something bad           taylor swift    emotion\n19  wish you were on it   florida georgia line    sadness",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_name</th>\n      <th>artist_name</th>\n      <th>pred_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>freeze me</td>\n      <td>death from above 1979</td>\n      <td>dark</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>hole in your heart</td>\n      <td>royal blood</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>the heat</td>\n      <td>the score</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>hollow bones pt. 1</td>\n      <td>rival sons</td>\n      <td>dark</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>i did something bad</td>\n      <td>taylor swift</td>\n      <td>emotion</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>wish you were on it</td>\n      <td>florida georgia line</td>\n      <td>sadness</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week3 liked songs:\n"
     ]
    },
    {
     "data": {
      "text/plain": "                   track_name              artist_name pred_topic\n2          feet don't fail me  queens of the stone age       dark\n3                     coconut                shag rock    sadness\n9                    castaway           brett eldredge    sadness\n12  the struggle discontinues            damian marley  lifestyle\n13                     6 a.m.           lester nowhere    sadness\n16               about a bird        fantastic negrito       dark\n19           will you be mine              anita baker    sadness",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_name</th>\n      <th>artist_name</th>\n      <th>pred_topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>feet don't fail me</td>\n      <td>queens of the stone age</td>\n      <td>dark</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>coconut</td>\n      <td>shag rock</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>castaway</td>\n      <td>brett eldredge</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>the struggle discontinues</td>\n      <td>damian marley</td>\n      <td>lifestyle</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>6 a.m.</td>\n      <td>lester nowhere</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>about a bird</td>\n      <td>fantastic negrito</td>\n      <td>dark</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>will you be mine</td>\n      <td>anita baker</td>\n      <td>sadness</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Load and clean songs\n",
    "songs = pd.read_csv('dataset.tsv', sep='\\t')\n",
    "def clean_text(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'[^a-z0-9\\s]', ' ', doc)\n",
    "    return re.sub(r'\\s+', ' ', doc).strip()\n",
    "songs['clean_doc'] = songs['lyrics'].astype(str).apply(clean_text)\n",
    "songs = songs.sort_values('release_date').reset_index(drop=True)\n",
    "\n",
    "# Train topic classifier on Weeks1-3\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "                             stop_words='english', max_features=500)),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "clf.fit(songs['clean_doc'][:750], songs['topic'][:750])\n",
    "songs['pred_topic'] = clf.predict(songs['clean_doc'])\n",
    "\n",
    "# Sample N songs from each week and display for manual liking\n",
    "N = 20\n",
    "week_samples = {}\n",
    "for i, week in enumerate(['Week1','Week2','Week3']):\n",
    "    \n",
    "    block = songs.iloc[i*250:(i+1)*250].copy()\n",
    "    sample = block.sample(N, random_state=42).reset_index()\n",
    "    sample['liked'] = False\n",
    "    week_samples[week] = sample\n",
    "    # pd.set_option('display.max_rows', 20)\n",
    "    # display(sample[['track_name','artist_name','pred_topic']])\n",
    "    print(sample[['track_name','artist_name','pred_topic']].to_string(index=False))\n",
    "\n",
    "# Manually mark liked songs:\n",
    "liked_indices = {\n",
    "    'Week1': [0,1,3,9,11,13],  \n",
    "    'Week2': [0,7,10,17,18,19],\n",
    "    'Week3': [2,3,9,12,13,16,19]\n",
    "}\n",
    "\n",
    "# Apply the manual likes\n",
    "for week, idxs in liked_indices.items():\n",
    "    df = week_samples[week]\n",
    "    df.loc[idxs, 'liked'] = True\n",
    "    week_samples[week] = df\n",
    "    print(f\"{week} liked songs:\")\n",
    "    display(df.loc[df['liked'], ['track_name','artist_name','pred_topic']])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:27.068604900Z",
     "start_time": "2025-06-27T16:08:26.931612100Z"
    }
   },
   "id": "fc4912f0d4dca6d8"
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     track_name                 artist_name pred_topic    score\n",
      "                            i did something bad                taylor swift    emotion 0.776444\n",
      "                                        so good                    dispatch    emotion 0.702704\n",
      "                                   feeling good                 demun jones    emotion 0.418033\n",
      "                               take on anything                  rebelution    emotion 0.271187\n",
      "nothing breaks like a heart (feat. miley cyrus)                 mark ronson    sadness 0.216591\n",
      "                                      strangers         albert hammond, jr.    emotion 0.202083\n",
      "                                     never ever                caro emerald    sadness 0.193474\n",
      "                                    to the moon                       phora    sadness 0.191108\n",
      "                                    without you               anderson east       dark 0.186526\n",
      "                                     tek it off                     jahmiel    emotion 0.179132\n",
      "                                  king of bones black rebel motorcycle club       dark 0.166935\n",
      "                                   skin & bones              eli young band       dark 0.166328\n",
      "         that song that we used to make love to            carrie underwood  lifestyle 0.155673\n",
      "                                  the dark side                        muse    sadness 0.148183\n",
      "                                         cushty                        ajmw    sadness 0.147308\n",
      "                                   cold blooded                      khalid       dark 0.144587\n",
      "                          that's how ya left me                 riley green    sadness 0.141083\n",
      "                                      graveyard            kelsea ballerini    sadness 0.138787\n",
      "                             at least you cried                     midland    sadness 0.137789\n",
      "                          love it if we made it                    the 1975       dark 0.136462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Aggregate all liked songs from Weeks 1–3\n",
    "liked_all = pd.concat([df[df['liked']] for df in week_samples.values()], ignore_index=True)\n",
    "\n",
    "# Train a TF–IDF vectorizer for each topic using Week 1–3 data\n",
    "train = songs.iloc[:750]\n",
    "topic_tfidf = {}\n",
    "for t in train['pred_topic'].unique():\n",
    "    docs = train[train['pred_topic'] == t]['clean_doc']\n",
    "    topic_tfidf[t] = TfidfVectorizer(\n",
    "        token_pattern=r'(?u)\\b\\w\\w+\\b',\n",
    "        stop_words='english'\n",
    "    ).fit(docs)\n",
    "\n",
    "# Build Andrew’s multi-topic profile vectors by merging liked docs per topic\n",
    "profile_vecs = {}\n",
    "for t in liked_all['pred_topic'].unique():\n",
    "    merged = ' '.join(liked_all[liked_all['pred_topic'] == t]['clean_doc'])\n",
    "    profile_vecs[t] = topic_tfidf[t].transform([merged])\n",
    "\n",
    "# Score Week 4 songs: compute cosine similarity between each song and the profile vector of its predicted topic\n",
    "week4 = songs.iloc[750:1000].copy()\n",
    "def score_row(row):\n",
    "    t = row['pred_topic']\n",
    "    if t in profile_vecs:\n",
    "        song_vec = topic_tfidf[t].transform([row['clean_doc']])\n",
    "        return cosine_similarity(song_vec, profile_vecs[t])[0, 0]\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "week4['score'] = week4.apply(score_row, axis=1)\n",
    "top20 = week4.nlargest(20, 'score')\n",
    "\n",
    "# Display the top 20 recommendations\n",
    "print(top20[['track_name', 'artist_name', 'pred_topic', 'score']].to_string(index=False))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-27T16:08:27.272110500Z",
     "start_time": "2025-06-27T16:08:27.071444400Z"
    }
   },
   "id": "741b6cfc2cc08752"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad5dd02947fb10b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
