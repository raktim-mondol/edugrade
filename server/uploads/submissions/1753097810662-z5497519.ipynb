{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b08c1b",
   "metadata": {},
   "source": [
    "# COMP9727 Assignment 25T2: Content-Based Music Recommendation\n",
    "Written by Jiacheng Nan (z5497519) on 11/06/2025\n",
    "\n",
    "This assignment is to aim to build a classic content-based music recommender from scratch, which uses the user's personal music interests and preferences to recommend a personalized playlist. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c13928",
   "metadata": {},
   "source": [
    "## Environment and Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3ff685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jiacheng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jiacheng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# General and text processing imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK data setup\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Models\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Validation\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score,f1_score, make_scorer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c6c18",
   "metadata": {},
   "source": [
    "## Part 1 Topic Classification\n",
    "### 1. Regex Preprocessing Comparison and Cross-Validation\n",
    "In the tutorial, the regular expression regex `[^\\w\\s]` was used for text preprocessing. This pattern removes all punctuation and special symbols, preserving only alphanumeric characters and whitespace. While effective for producing normalized tokens, this method may be overly aggressive for certain natural language processing tasks.\n",
    "\n",
    "In particular, punctuation marks such as `&`, `=`, and `?` can carry important semantic meaning, especially in informal or creative text such as song lyrics. The removal of these symbols may degrade classification performance, as such tokens can contribute to the uniqueness of an artist’s style or lyrical theme. Moreover, since text is lowercased during preprocessing, including uppercase characters in the allowed token set is redundant and may introduce unnecessary noise.\n",
    "\n",
    "To assess the impact of punctuation retention, two preprocessing strategies were implemented:\n",
    "- Remove all punctuation, remaining the same as the tutorial's setup\n",
    "- Retain key punctuation marks and only include lower case letters\n",
    "\n",
    "Both strategies were evaluated using two classification models: **Multinomial Naive Bayes (MNB)** and **Bernoulli Naive Bayes (BNB)**. Performance was assessed using standard metrics: `accuracy`, `macro-f1`, and `micro-f1`.\n",
    "\n",
    "Unlike the tutorial, which relied on a single `train_test_split()`, this evaluation used **Stratified 5-Fold Cross-Validation** via `StratifiedKFold()`. This method was chosen to better accommodate potential class imbalance across the five topic labels, and to yield more stable and generalizable performance estimates. Stratification ensures that each fold maintains similar class distributions, while cross-validation reduces the risk of performance fluctuation due to random splits, which is common with fixed train-test divisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff5251",
   "metadata": {},
   "source": [
    "#### 1.1 Preprocessing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041a60e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and combine the coloumns into a single text column\n",
    "df = pd.read_csv(\"dataset.tsv\", sep='\\t')\n",
    "df = df.drop_duplicates().dropna()\n",
    "categories = ['artist_name', 'track_name', 'release_date', 'genre', 'lyrics', 'topic']\n",
    "df['content'] = df[categories].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Load stopwords and initialize stemmer\n",
    "stop_words_NLTK = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text, regex, stop_words, lowercase=True, stemming=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(regex, '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    if stemming:\n",
    "        tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be225c2",
   "metadata": {},
   "source": [
    "#### 1.2 Remove all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c64d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_A'] = df['content'].apply(lambda c: preprocess_text(c, regex=r\"[^\\w\\s]\", stop_words=stop_words_NLTK))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d38562",
   "metadata": {},
   "source": [
    "#### 1.3 Retain key punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2274bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_B'] = df['content'].apply(lambda c:preprocess_text(c, regex = r'[^a-z\\s!\"#$%&\\'()*+,\\-./:;<=>?@[\\\\\\]^_`{|}~]', stop_words=stop_words_NLTK))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79792b3d",
   "metadata": {},
   "source": [
    "#### 1.4 Applying Preprocessing and Run Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24541ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Punctuation:\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877027      0.008705       0.852283      0.015367   \n",
      "1   BNB       0.913514      0.008705       0.760820      0.011860   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877027      0.008705  \n",
      "1       0.913514      0.008705   \n",
      "\n",
      "Keeping Punctuations and lower case:\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877027      0.012926       0.854101      0.019455   \n",
      "1   BNB       0.913514      0.006266       0.761430      0.009896   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877027      0.012926  \n",
      "1       0.913514      0.006266   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation function for cross-validation\n",
    "def evaluate_model(texts, labels, models, token_pattern, max_features):\n",
    "    # Vectorize\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    vectorizer = CountVectorizer(max_features=max_features, token_pattern=token_pattern)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    y = labels\n",
    "    records = []\n",
    "\n",
    "    scorers = {\n",
    "        'Accuracy': make_scorer(accuracy_score),\n",
    "        'Macro-F1': make_scorer(f1_score, average='macro'),\n",
    "        'Micro-F1': make_scorer(f1_score, average='micro')\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Cross-validate the model\n",
    "        scores = cross_validate(model, X, y, cv=cv, scoring=scorers)\n",
    "        row = {'Model': model_name}\n",
    "        for metric in scorers:\n",
    "            row[f'{metric} mean'] = scores[f'test_{metric}'].mean()\n",
    "            row[f'{metric} std']  = scores[f'test_{metric}'].std()\n",
    "        records.append(row)\n",
    "\n",
    "    print(pd.DataFrame(records),\"\\n\")\n",
    "\n",
    "# Run comparison\n",
    "models = {'MNB': MultinomialNB(),'BNB': BernoulliNB()}\n",
    "token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "print(\"No Punctuation:\")\n",
    "evaluate_model(df['content_A'], df['topic'], models, token_pattern, max_features=3000)\n",
    "\n",
    "print(\"Keeping Punctuations and lower case:\")\n",
    "evaluate_model(df['content_B'], df['topic'], models, token_pattern, max_features=3000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4347cd2",
   "metadata": {},
   "source": [
    "#### 1.5 Results\n",
    "Retaining selected punctuation produced a slight but consistent improvement in `macro-F1` scores across both models:\n",
    "- **MNB**: from 0.852283 to 0.854101\n",
    "- **BNB**: from 0.760820 to 0.761430\n",
    "\n",
    "In contrast,`accuracy` and `micro-F1` scores remained unchanged, with overlapping standard deviations indicating negligible effect. Although the improvements in `macro-F1` were modest and within potential variance, the retention of punctuation did not negatively impact performance.\n",
    "\n",
    "Based on these findings, the second preprocessing strategy of **retaining key punctuation and limiting input to lowercase letter** was selected for all further experiments. This approach preserves potentially meaningful symbols in the lyrical data while maintaining a consistent token structure for vectorization and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33506ce4",
   "metadata": {},
   "source": [
    "### 2. Developing a Multinomial Naive Bayes Model and Further Tuning\n",
    "Following the baseline implementation of both MNB and BNB models, the next objective was to enhance the pipeline by optimizing its preprocessing components. The experiement focused on varying key factors to assess their influence on model performance, which are:\n",
    "- **Special character removal**: varying the regex to control which symbols are stripped or retained (extending the earlier comparison to include more variations)\n",
    "- **Token definition**: defining what a “word” is   \n",
    "- **Stopword list selection**: comparing the performance of NLTK’s stop-word list, scikit-learn’s built-in list, or no stop-words at all  \n",
    "- **Case normalization and stemming**: evaluating the effect of converting all text to lowercase and applying Porter stemming\n",
    "\n",
    "Each combination of preprocessing choices was assessed using 5-fold stratified cross-validation, with performance evaluated based on `accuracy`, `macro-f1`, and `micro-f1` scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659406ac",
   "metadata": {},
   "source": [
    "#### 2.1 Further Special Character Removal\n",
    "Although retaining key punctuation provided a modest improvement, not all symbols contribute equally to classification performance. Many special characters occur so infrequently in the dataset that preserving them increases complexity without yielding benefits. To identify the most impactful symbols, each punctuation mark was removed individually, and the corresponding change in model performance was recorded. This experiment guided the construction of a regular expression that retains only those characters demonstrating a positive effect on the accuracy in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d5eafbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877027      0.012926       0.854101      0.019455   \n",
      "1   BNB       0.913514      0.006266       0.761430      0.009896   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877027      0.012926  \n",
      "1       0.913514      0.006266   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].apply(lambda c:preprocess_text(c, regex = r\"[^a-z\\s.'\\/-]\", stop_words=stop_words_NLTK))\n",
    "evaluate_model(df['content'], df['topic'], models, token_pattern, max_features=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7c52d",
   "metadata": {},
   "source": [
    "After systematically evaluating the impact of each punctuation mark, only those whose removal led to a measurable performance drop, namely ``.``, `'`, `/`, `-` and alphabet `a-z`, were retained in the final regular expression. All other symbols were excluded to simplify the preprocessing pipeline and reduce noise. Although the overall metrics remain unchanged from Section 1, this optimized regex yields a cleaner and more efficient preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730460a",
   "metadata": {},
   "source": [
    "#### 2.2 Token Definition\n",
    "Tokenization divides text into discrete units called tokens, forming the basis of any NLP pipeline. In scikit-learn’s `CountVectorizer()`, the `token_pattern` parameter, defined by a regular expression, determines which character sequences are recognized as valid tokens. The choice of pattern can substantially affect model performance.\n",
    "\n",
    "To determine the most effective token definition, several patterns were compared:\n",
    "- **Only letters**: exclude digits entirely, retaining only alphabetic characters\n",
    "- **Punctuation handling**  \n",
    "  - **Default**: use scikit-learn’s standard pattern `(\\b\\w\\w+\\b)`, which accepts words of length two or more, including digits and underscores. (this excludes single digits)\n",
    "  - **Keep key punctuations**: allow selected symbols, `&`, `?`, `!`, `'`, `.`, within tokens  \n",
    "  - **Hyphen words**: treat sequences like `well-known` or `state-of-the-art` as single tokens by permitting internal hyphens\n",
    "  - **Full punctuation**: keeping a broader range of symbols within tokens\n",
    "- **Minimum token length**: lower the minimum token length to one character, which includs digits\n",
    "\n",
    "Each variant was applied in turn, and the resulting classification performance of `accuracy`, `macro-F1`, and `micro-F1` scores was recorded using **stratified 5-fold cross-validation**. The pattern yielding the highest scores was selected for the final pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c194092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern only_alphabet:\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.876351      0.012926       0.852908      0.020423   \n",
      "1   BNB       0.913514      0.006957       0.761101      0.011814   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.876351      0.012926  \n",
      "1       0.913514      0.006957   \n",
      "\n",
      "Pattern default:\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877027      0.012926       0.854101      0.019455   \n",
      "1   BNB       0.913514      0.006266       0.761430      0.009896   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877027      0.012926  \n",
      "1       0.913514      0.006266   \n",
      "\n",
      "Pattern keep_key_punct:\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877703      0.013913       0.854947      0.021507   \n",
      "1   BNB       0.913514      0.006266       0.761375      0.009835   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877703      0.013913  \n",
      "1       0.913514      0.006266   \n",
      "\n",
      "Pattern hyphen_words:\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877027      0.011819       0.854196      0.019828   \n",
      "1   BNB       0.914189      0.007584       0.761536      0.011782   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877027      0.011819  \n",
      "1       0.914189      0.007584   \n",
      "\n",
      "Pattern full_punct:\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877027      0.010811       0.854187      0.019202   \n",
      "1   BNB       0.913514      0.006957       0.761325      0.011082   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877027      0.010811  \n",
      "1       0.913514      0.006957   \n",
      "\n",
      "Pattern allowing_single_char:\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.876351      0.012926       0.852908      0.020423   \n",
      "1   BNB       0.913514      0.006957       0.761101      0.011814   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.876351      0.012926  \n",
      "1       0.913514      0.006957   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_patterns = {\n",
    "    'only_alphabet': r\"(?u)\\b[a-zA-Z]+\\b\",\n",
    "    'default': r\"(?u)\\b\\w\\w+\\b\",\n",
    "    'keep_key_punct': r\"(?u)\\b[\\w'&!?\\.]+\\b\", \n",
    "    'hyphen_words': r\"(?u)\\b[a-zA-Z]+(?:-[a-zA-Z]+)*\\b\",\n",
    "    'full_punct': r\"(?u)\\b[\\w!\\\"#$%&'()*+,\\-./:;<=>?@\\[\\]\\\\^_`{|}~]+\\b\",\n",
    "    'allowing_single_char': r\"(?u)\\b\\w+\\b\",\n",
    "}\n",
    "\n",
    "for name, pattern in token_patterns.items():\n",
    "    print(f\"Pattern {name}:\")\n",
    "    evaluate_model(df['content'], df['topic'], models, pattern, max_features=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98283b09",
   "metadata": {},
   "source": [
    "The results indicate that the `keep_key_punct` pattern provided the most consistent benefit for MNB, increasing accuracy from **0.8770** to **0.8777**, with corresponding small gains in both macro-F1 and micro-F1. In contrast, the `hyphenated-word` pattern yielded the highest accuracy for BNB, improving from **0.9135** to **0.9142**, while having negligible impact on MNB. All other patterns either produced no improvement or slightly degraded performance.\n",
    "\n",
    "A combined token pattern (retaining both key punctuation and hyphens) did not enhance either classifier further. To select a single pattern for subsequent experiments, the accuracy means of both MNB and BNB under each pattern were summed. The `keep_key_punct` pattern achieved the highest aggregate score and was therefore adopted for all future tuning in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f60537a",
   "metadata": {},
   "source": [
    "#### 2.3 Stopword List Selection\n",
    "In natural language text, very common words, such as articles, prepositions, and pronouns, often carry little semantic value and are therefore filtered out as **stopwords**. Removing stopwords can reduce noise and shrink the feature space, improving model efficiency. However, in tasks involving short or specialized texts, excessive removal may discard informative tokens and harm performance.\n",
    "\n",
    "To determine the optimal stopword configuration, three options were compared:\n",
    "- **No stopwords**: keep every single token\n",
    "- **NLTK stopwords**: use the comprehensive list provided by NLTK, which includes common English words as well as several colloquial and dialectal terms\n",
    "- **scikit-learn stopwords**: use the built-in list from scikit-learn, which is smaller and more conservative, focusing on standard grammatical words and excluding some colloquial terms\n",
    "\n",
    "Each configuration was evaluated via **stratified 5-fold cross-validation** using `accuracy`, `macro-F1`, and `micro-F1` metrics. The stopword list yielding the highest score was selected for the final pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c624f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopword List: no_stopwords\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877703      0.011585       0.854195      0.018993   \n",
      "1   BNB       0.913514      0.008164       0.761449      0.010366   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877703      0.011585  \n",
      "1       0.913514      0.008164   \n",
      "\n",
      "Stopword List: nltk\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877703      0.010978       0.854084      0.016864   \n",
      "1   BNB       0.913514      0.008164       0.761473      0.010367   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877703      0.010978  \n",
      "1       0.913514      0.008164   \n",
      "\n",
      "Stopword List: sklearn\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.870946      0.009165       0.835782      0.012011   \n",
      "1   BNB       0.911486      0.008913       0.760498      0.010589   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.870946      0.009165  \n",
      "1       0.911486      0.008913   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying updates from previous section to originial content\n",
    "df['content'] = df['content'].apply(lambda c:preprocess_text(c, regex = r\"[^a-z\\s!&?\\.]\", stop_words=stop_words_NLTK))\n",
    "\n",
    "#　Creating raw content for further processing\n",
    "df['raw_content'] = df['content']\n",
    "stopword_lists = {\n",
    "    'no_stopwords': set(),\n",
    "    'nltk': set(stopwords.words('english')),\n",
    "    'sklearn': set(ENGLISH_STOP_WORDS)\n",
    "}\n",
    "regex = r\"[^a-z\\s.'\\/-]\"\n",
    "token_pattern = r\"(?u)\\b[\\w'&!?\\.]+\\b\"\n",
    "\n",
    "for name, stop_word in stopword_lists.items():\n",
    "    df['raw_content'] = df['content'].apply(lambda c: preprocess_text(c, regex=regex, stop_words=stop_word))\n",
    "    print(f\"Stopword List: {name}\")\n",
    "    evaluate_model(df['raw_content'], df['topic'], models, token_pattern, max_features=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b95f64",
   "metadata": {},
   "source": [
    "From the results, it is evident that omitting stopword removal entirely and using NLTK’s stopword list yielded identical performance for both classifiers. In contrast, applying scikit-learn’s stopword list reduced performance.\n",
    "\n",
    "Because scikit-learn’s stopword set degraded both models and NLTK’s list maintained equivalent results to keeping all tokens, **NLTK’s stopword list** was selected as the optimal choice. This list will be used in the final preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83919aa8",
   "metadata": {},
   "source": [
    "#### 2.4 Case Normalization & Stemming\n",
    "Two final preprocessing steps were evaluated:\n",
    "1. **Case normalization**: converting all text to lowercase and collapsing same words into the same token, reducing sparsity, but may lose useful distinctions in proper nouns or acronyms\n",
    "2. **Stemming**: applying a Porter stemmer reduces words to their root form, which can improve recall by grouping inflected forms, though it may over-simplify certain tokens\n",
    "\n",
    "Four pipeline configurations were tested:\n",
    "- **Scenario A: Lowercase and Stemming**  \n",
    "- **Scenario B: Lowercase only**  \n",
    "- **Scenario C: Stemming only**  \n",
    "- **Scenario D: No lowercase, no stemming**  \n",
    "\n",
    "Each scenario was evaluated using **stratified 5-fold cross-validation** and the metrics `accuracy`, `macro-F1`, and `micro-F1`. The configuration achieving the highest score was adopted for all subsequent experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "067ec146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario A: lowercase_and_stemming: \n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877703      0.010978       0.854084      0.016864   \n",
      "1   BNB       0.913514      0.008164       0.761473      0.010367   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877703      0.010978  \n",
      "1       0.913514      0.008164   \n",
      "\n",
      "Scenario B: lowercase_only: \n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.878378      0.010468       0.853753      0.017911   \n",
      "1   BNB       0.913514      0.006620       0.761719      0.010495   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.878378      0.010468  \n",
      "1       0.913514      0.006620   \n",
      "\n",
      "Scenario C: stemming_only: \n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877703      0.010978       0.854084      0.016864   \n",
      "1   BNB       0.913514      0.008164       0.761473      0.010367   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877703      0.010978  \n",
      "1       0.913514      0.008164   \n",
      "\n",
      "Scenario D: nothing: \n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.878378      0.010468       0.853753      0.017911   \n",
      "1   BNB       0.913514      0.006620       0.761719      0.010495   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.878378      0.010468  \n",
      "1       0.913514      0.006620   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scenarios = [\n",
    "    ('A: lowercase_and_stemming', True,  True),\n",
    "    ('B: lowercase_only', True,  False),\n",
    "    ('C: stemming_only', False, True),\n",
    "    ('D: nothing', False, False)\n",
    "]\n",
    "\n",
    "df['raw_content'] = df['content']\n",
    "stop_word = stop_words_NLTK\n",
    "\n",
    "for label, lower, stem in scenarios:\n",
    "    processed = df['raw_content'].apply(\n",
    "        lambda c: preprocess_text(\n",
    "            c,\n",
    "            regex=regex,\n",
    "            stop_words=stop_word,\n",
    "            lowercase=lower,\n",
    "            stemming=stem\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"Scenario {label}: \")\n",
    "    evaluate_model(processed, df['topic'], models, token_pattern, max_features=3000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a708f0",
   "metadata": {},
   "source": [
    "From the results above, the following observations were made:\n",
    "- **Scenarios B and D** yielded identical, lower performance metrics\n",
    "- **Scenarios A and C** achieved the highest scores, matching the best results from Section 2.3\n",
    "\n",
    "Although Scenario C (stemming only) matched Scenario A in performance, leaving out lowercase conversion retains redundant uppercase tokens, which adds noise and sparsity in a bag-of-words model. Lowercasing is an inexpensive normalization step that unifies token variants without harming accuracy. Accordingly, **Scenario A (lowercase and stemming)** was selected for the final preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e9291",
   "metadata": {},
   "source": [
    "### 3. Compare BNB and MNB with 5-Fold Cross-Validation\n",
    "This section presents a performance comparison between **BernoulliNB** and **MultinomialNB** using the fully tuned preprocessing pipeline from Sections 1 and 2. To obtain robust and unbiased estimates on the **imbalanced** topic dataset, **stratified 5-fold cross-validation** was employed.\n",
    "\n",
    "Models were evaluated according to **accuracy** and **F1 score** metrics. Although accuracy provides an overall measure of correctness, it may be inflated by the majority classes in imbalanced datasets. Consequently, **macro–average F1** was selected as the primary metric, since it equally weights all classes and emphasizes performance on minority categories. By analyzing these metrics, the superior classifier for topic prediction in this assignment will be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2af6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877703      0.010978       0.854084      0.016864   \n",
      "1   BNB       0.913514      0.008164       0.761473      0.010367   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877703      0.010978  \n",
      "1       0.913514      0.008164   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex = r\"[^a-z\\s.'\\/-]\"\n",
    "token_pattern = r\"(?u)\\b[\\w'&!?\\.]+\\b\"\n",
    "stop_word = stop_words_NLTK\n",
    "\n",
    "df['content'] = df['content'].apply(\n",
    "    lambda c: preprocess_text(\n",
    "        c,\n",
    "        regex=regex,\n",
    "        stop_words=stop_word,\n",
    "        lowercase=True,\n",
    "        stemming=True\n",
    "    )\n",
    ")\n",
    "\n",
    "evaluate_model(df['content'], df['topic'], models, token_pattern, max_features=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49f2b9",
   "metadata": {},
   "source": [
    "#### 3.1 Results\n",
    "BNB encodes each token as a binary feature (present or absent), while MNB additionally leverages term frequencies. The stratified 5-fold cross-validation results reveal:\n",
    "- **Accuracy**: BNB achieved higher overall `accuracy`, since its binary representation often over-predicts majority classes and thus records more correct predictions in raw counts\n",
    "\n",
    "- **Macro-F1**: MNB outperformed BNB on `macro-F1`. By modeling word counts, MNB more effectively distinguishes minority topic patterns and reduces majority class bias.\n",
    "\n",
    "Given the imbalanced distribution of the five topics, **macro-F1** provides a more equitable measure of performance, as it assigns equal weight to each class. Therefore, **MultinomialNB** was chosen as the preferred classifier for all subsequent topic classification experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bcbfd",
   "metadata": {},
   "source": [
    "### 4. Number of Features\n",
    "The size of feature words in `CountVectorizer()` is a critical hyperparameter. Too few features may exclude discriminative terms essential for topic separation, and too many features can introduce noise, increase sparsity, and risk overfitting.\n",
    "\n",
    "To identify the optimal balance, both MNB and BNB were evaluated over a sequence of feature word sizes, ranging from **1** to **10,000** features in increments of **1,000**. For each setting, models were trained and assessed using **stratified 5-fold cross-validation**, and performance was recorded in terms of **accuracy** and **micro-F1**.\n",
    "\n",
    "The resulting performance curves were analyzed to select the feature words size that maximized generalization while avoiding unnecessary complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8203c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features: 1\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.329054      0.001655       0.099034      0.000375   \n",
      "1   BNB       0.325000      0.005812       0.123347      0.029882   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.329054      0.001655  \n",
      "1       0.325000      0.005812   \n",
      "\n",
      "Max Features: 1001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.908108      0.013067       0.890328      0.018687   \n",
      "1   BNB       0.968243      0.009698       0.934940      0.025051   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.908108      0.013067  \n",
      "1       0.968243      0.009698   \n",
      "\n",
      "Max Features: 2001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.886486      0.010598       0.865110      0.011559   \n",
      "1   BNB       0.925676      0.006410       0.797859      0.012435   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.886486      0.010598  \n",
      "1       0.925676      0.006410   \n",
      "\n",
      "Max Features: 3001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877703      0.010978       0.854084      0.016864   \n",
      "1   BNB       0.913514      0.008164       0.761473      0.010367   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877703      0.010978  \n",
      "1       0.913514      0.008164   \n",
      "\n",
      "Max Features: 4001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.877703      0.008385       0.848938      0.010999   \n",
      "1   BNB       0.897297      0.012568       0.737599      0.012614   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.877703      0.008385  \n",
      "1       0.897297      0.012568   \n",
      "\n",
      "Max Features: 5001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.874324      0.008108       0.843615      0.008836   \n",
      "1   BNB       0.879054      0.006891       0.714524      0.012786   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.874324      0.008108  \n",
      "1       0.879054      0.006891   \n",
      "\n",
      "Max Features: 6001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.870270      0.004054       0.832933      0.004097   \n",
      "1   BNB       0.857432      0.012532       0.681526      0.017146   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.870270      0.004054  \n",
      "1       0.857432      0.012532   \n",
      "\n",
      "Max Features: 7001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.871622      0.007704       0.831312      0.007183   \n",
      "1   BNB       0.828378      0.013412       0.636557      0.013106   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.871622      0.007704  \n",
      "1       0.828378      0.013412   \n",
      "\n",
      "Max Features: 8001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.866216      0.006957       0.816944      0.006492   \n",
      "1   BNB       0.809459      0.010811       0.598937      0.013805   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.866216      0.006957  \n",
      "1       0.809459      0.010811   \n",
      "\n",
      "Max Features: 9001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.862838      0.007584       0.808370      0.015049   \n",
      "1   BNB       0.799324      0.009930       0.581751      0.001734   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.862838      0.007584  \n",
      "1       0.799324      0.009930   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_features = range(1, 10001, 1000)\n",
    "df['raw_content'] = df['content']\n",
    "\n",
    "for max_features in num_of_features:\n",
    "    print(f\"Max Features: {max_features}\")\n",
    "    evaluate_model(df['raw_content'], df['topic'], models, token_pattern, max_features=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02087e",
   "metadata": {},
   "source": [
    "#### 4.1 Narrowing Feature Size\n",
    "The results indicated that performance peaked at a size of approximately **1,001** features. Beyond **2,001** features, all evaluation metrics began to decline gradually. This pattern suggests that the optimal size lies within the range **1,001 to 2,001**. To pinpoint the exact range, the feature range was narrowed to **1,001 to 2,001**, using increments of **100** for a more specific search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc40b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Features: 1\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.329054      0.001655       0.099034      0.000375   \n",
      "1   BNB       0.325000      0.005812       0.123347      0.029882   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.329054      0.001655  \n",
      "1       0.325000      0.005812   \n",
      "\n",
      "Max Features: 101\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.863514      0.016770       0.824916      0.025341   \n",
      "1   BNB       0.931081      0.008705       0.831388      0.030741   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.863514      0.016770  \n",
      "1       0.931081      0.008705   \n",
      "\n",
      "Max Features: 201\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.913514      0.009698       0.888709      0.018658   \n",
      "1   BNB       0.979054      0.011585       0.955639      0.026281   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.913514      0.009698  \n",
      "1       0.979054      0.011585   \n",
      "\n",
      "Max Features: 301\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.929730      0.008653       0.918843      0.013858   \n",
      "1   BNB       0.990541      0.007524       0.987201      0.010750   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.929730      0.008653  \n",
      "1       0.990541      0.007524   \n",
      "\n",
      "Max Features: 401\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.932432      0.006410       0.917102      0.012442   \n",
      "1   BNB       0.985135      0.010158       0.977985      0.016154   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.932432      0.006410  \n",
      "1       0.985135      0.010158   \n",
      "\n",
      "Max Features: 501\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.937838      0.008164       0.918929      0.013443   \n",
      "1   BNB       0.985811      0.010768       0.978482      0.016669   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.937838      0.008164  \n",
      "1       0.985811      0.010768   \n",
      "\n",
      "Max Features: 601\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.925000      0.008108       0.905378      0.019874   \n",
      "1   BNB       0.981081      0.012568       0.971204      0.019544   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.925000      0.008108  \n",
      "1       0.981081      0.012568   \n",
      "\n",
      "Max Features: 701\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.922973      0.008108        0.90656      0.012762   \n",
      "1   BNB       0.980405      0.013581        0.97050      0.020418   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.922973      0.008108  \n",
      "1       0.980405      0.013581   \n",
      "\n",
      "Max Features: 801\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.916216      0.012162       0.899457      0.017174   \n",
      "1   BNB       0.974324      0.013614       0.958796      0.021670   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.916216      0.012162  \n",
      "1       0.974324      0.013614   \n",
      "\n",
      "Max Features: 901\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.910135      0.013781       0.892588      0.020363   \n",
      "1   BNB       0.972297      0.011973       0.951152      0.020031   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.910135      0.013781  \n",
      "1       0.972297      0.011973   \n",
      "\n",
      "Max Features: 1001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.908108      0.013067       0.890328      0.018687   \n",
      "1   BNB       0.968243      0.009698       0.934940      0.025051   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.908108      0.013067  \n",
      "1       0.968243      0.009698   \n",
      "\n",
      "Max Features: 1101\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.901351      0.012348       0.882639      0.015333   \n",
      "1   BNB       0.962838      0.009314       0.922294      0.026272   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.901351      0.012348  \n",
      "1       0.962838      0.009314   \n",
      "\n",
      "Max Features: 1201\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.904730      0.014076       0.890171      0.017526   \n",
      "1   BNB       0.958108      0.007277       0.912960      0.018855   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.904730      0.014076  \n",
      "1       0.958108      0.007277   \n",
      "\n",
      "Max Features: 1301\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.899324      0.016328       0.884284      0.021999   \n",
      "1   BNB       0.953378      0.006891       0.899091      0.020891   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.899324      0.016328  \n",
      "1       0.953378      0.006891   \n",
      "\n",
      "Max Features: 1401\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.899324      0.015614       0.883086      0.022076   \n",
      "1   BNB       0.949324      0.007402       0.884957      0.034854   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.899324      0.015614  \n",
      "1       0.949324      0.007402   \n",
      "\n",
      "Max Features: 1501\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.900676      0.016356       0.885564      0.021529   \n",
      "1   BNB       0.945270      0.006891       0.875759      0.028296   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.900676      0.016356  \n",
      "1       0.945270      0.006891   \n",
      "\n",
      "Max Features: 1601\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.897297      0.014428       0.880318      0.016986   \n",
      "1   BNB       0.943243      0.006891       0.870783      0.036023   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.897297      0.014428  \n",
      "1       0.943243      0.006891   \n",
      "\n",
      "Max Features: 1701\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.893243      0.011225       0.873690      0.012653   \n",
      "1   BNB       0.939865      0.006891       0.859643      0.030166   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.893243      0.011225  \n",
      "1       0.939865      0.006891   \n",
      "\n",
      "Max Features: 1801\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.889189      0.010554       0.869486      0.014553   \n",
      "1   BNB       0.934459      0.005056       0.831365      0.022167   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.889189      0.010554  \n",
      "1       0.934459      0.005056   \n",
      "\n",
      "Max Features: 1901\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.887838      0.011184       0.865881      0.014144   \n",
      "1   BNB       0.931081      0.006266       0.817022      0.012544   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.887838      0.011184  \n",
      "1       0.931081      0.006266   \n",
      "\n",
      "Max Features: 2001\n",
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.886486      0.010598       0.865110      0.011559   \n",
      "1   BNB       0.925676      0.006410       0.797859      0.012435   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.886486      0.010598  \n",
      "1       0.925676      0.006410   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_features = range(1, 2002, 100)\n",
    "df['raw_content'] = df['content']\n",
    "\n",
    "for max_features in num_of_features:\n",
    "    print(f\"Max Features: {max_features}\")\n",
    "    evaluate_model(df['raw_content'], df['topic'], models, token_pattern, max_features=max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad501dc",
   "metadata": {},
   "source": [
    "#### 4.2 Results\n",
    "As the feature word size decreased to **1 to 500**, both models showed consistent performance gains. The peak `macro-F1` scores occurred at 500 features, with BNB score of **0.9784** and MNB of **0.9189**.\n",
    "\n",
    "Beyond 500 features, all metrics began to decline. Therefore, a **maximum feature size of 500** was selected for all further experiments, as it maximizes model performance while minimizing extraneous features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005952a0",
   "metadata": {},
   "source": [
    "### 5. Support Vector Machines (LinearSVM)\n",
    "Support Vector Machines are supervised, margin-based classifiers that identify a hyperplane in high dimensional feature space to maximally separate classes. In text classification, each token corresponds to one dimension, resulting in very sparse vectors. A **linear SVM** often outperforms Naive Bayes models by emphasizing discriminative features and reducing the influence of noisy features.\n",
    "\n",
    "Given the lyrics dataset, represented as a 500-dimensional bag-of-words, linear SVM is well suited to this high dimensional, sparse setting. It offers greater flexibility than Naive Bayes in weighting combinations of tokens and has a strong record in NLP tasks.\n",
    "\n",
    "The `LinearSVC()` implementation requires tuning two hyperparameters. One of them is the `max_iter`, which is the maximum number of optimization iterations. The default of 1,000 iterations in scikit-learn may not achieve convergence, and triggering a warning. To ensure convergence, `max_iter` was increased in increments of **500** until no warnings occurred. The other hyperpartameter is the `dual`, which is a boolean flag that selects between solving the dual or the primal optimization problem. Even knowing that `dual=False` is typically faster when **n_samples > n_features**, both settings were evaluated to identify the best trade-off between efficiency and accuracy.\n",
    "\n",
    "The evaluation procedure mirrored Section 3, employing **stratified 5-fold cross-validation** and the metrics `accuracy` and `macro-F1`. It was hypothesized that linear SVM would match or exceed the performance of both BNB and MNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a540c8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Accuracy mean  Accuracy std  Macro-F1 mean  Macro-F1 std  \\\n",
      "0   MNB       0.935811      0.005653       0.916603      0.011561   \n",
      "1   BNB       0.986486      0.009791       0.978959      0.016404   \n",
      "2   SVM       0.970946      0.010598       0.957399      0.016039   \n",
      "\n",
      "   Micro-F1 mean  Micro-F1 std  \n",
      "0       0.935811      0.005653  \n",
      "1       0.986486      0.009791  \n",
      "2       0.970946      0.010598   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].apply(\n",
    "    lambda c: preprocess_text(\n",
    "        c,\n",
    "        regex=regex,\n",
    "        stop_words=stop_word,\n",
    "        lowercase=True,\n",
    "        stemming=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define models for evaluation\n",
    "models = {\n",
    "    'MNB': MultinomialNB(),\n",
    "    'BNB': BernoulliNB(),\n",
    "    'SVM': LinearSVC(max_iter=2500, dual=False)\n",
    "}\n",
    "\n",
    "evaluate_model(df['content'], df['topic'], models, token_pattern, max_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac07c6",
   "metadata": {},
   "source": [
    "#### 5.1 Results\n",
    "Through systematic tuning, convergence warnings disappeared once `max_iter` was set to **2,500**, and neither higher nor lower values improved its performance. Similarly, setting `dual=False` consistently outperformed `dual=True`. Therefore, the final SVM configuration used `max_iter=2500` and `dual=False`.\n",
    "\n",
    "When benchmarked alongside our Naive Bayes classifiers, the linear SVM did outperform MNB on all metrics but did not surpass BNB. This result diverges from our initial hypothesis and from the findings in Section 3, where MNB had demonstrated higher `macro-F1` score than BNB. The strong performance of BNB in this round is likely attributable to the reduced feature words size to 500, which produces a binary feature signal that BNB exploits more effectively.\n",
    "\n",
    "In conclusion, **Bernoulli Naive Bayes**, paired with the optimized preprocessing pipeline, was selected as the final classifier for all remaining experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ad60c",
   "metadata": {},
   "source": [
    "## Part 2 Recommendation Methods\n",
    "### 1. Building Recommendation System\n",
    "Having established and tuned the Bernoulli Naive Bayes classifier in Part 1, the next phase simulates a content-based recommendation system. This system leverages the topic predictions generated by BNB to suggest new songs tailored to individual user preferences.\n",
    "#### 1.1 Preprocessing Setup\n",
    "The preprocessing steps in this part is similar to Part 1. The only difference is that the dataset is divided into four, represented by weeks. Week 1 to 3 will be the training set and week 4 will be the test set. Details of the preprocessing steps (e.g., regex, token pattern, stopwords, stemming) are inherited from the final pipeline established in Part 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1d5299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and combine the coloumns into a single text column\n",
    "df = pd.read_csv(\"dataset.tsv\", sep='\\t')\n",
    "df = df.drop_duplicates().dropna()\n",
    "categories = ['artist_name', 'track_name', 'release_date', 'genre', 'lyrics']\n",
    "df['content'] = df[categories].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train = df.iloc[:750].copy()\n",
    "test = df.iloc[750:].copy()\n",
    "\n",
    "# Load best preprocessing parameters\n",
    "chosen_stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "chosen_regex = r\"[^a-z\\s.'\\/-]\"\n",
    "chosen_token_pattern = r\"(?u)\\b[\\w'&!?\\.]+\\b\"\n",
    "chosen_max_features = 500\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text_recommender(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(chosen_regex, '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in chosen_stop_words]\n",
    "    tokens = [ps.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train['content'] = train['content'].apply(preprocess_text_recommender)\n",
    "test['content'] = test['content'].apply(preprocess_text_recommender)\n",
    "\n",
    "# Vectorize and train set\n",
    "vectorizer = TfidfVectorizer(max_features=chosen_max_features, token_pattern=chosen_token_pattern)\n",
    "X_train = vectorizer.fit_transform(train['content'])\n",
    "y_train = train['topic']\n",
    "classifier = BernoulliNB().fit(X_train, y_train)\n",
    "\n",
    "# predict topics on train and test\n",
    "X_test = vectorizer.transform(test['content'])\n",
    "train['pred_topic'] = classifier.predict(X_train)\n",
    "test['pred_topic'] = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30ecee",
   "metadata": {},
   "source": [
    "#### 1.2 Building User Profile\n",
    "We will be building the user profile according to their music preferences in this section. To do so we will first take all the training songs from week 1 to 3 in given topics and mark as liked if the user's keywords for that topic matches the lyrics. Then we will fit the topic-specific TF-IDF model on all the songs in that topic, and transform them into one large document containing the lyrics of the liked songs concatenating into a single TF-IDF vector. \n",
    "With this process, the vector then become the user profile for that topic, highligthing the words occuring most often.\n",
    "\n",
    "Each user’s profile is constructed from their liked songs in Weeks 1–3 as follows:\n",
    "1. For each topic, select training songs whose predicted topic matches and whose lyrics contain at least one of the user’s topic-specific keywords.\n",
    "2. For each topic, train a `TfidfVectorizer()` on the preprocessed lyrics of all training songs predicted under that topic. This captures the term distributions characteristic of each topic.\n",
    "3. Connect the preprocessed lyrics of the user’s liked songs for each topic into a single document.\n",
    "4. Use the corresponding topic’s TF-IDF model to convert the document into a single profile vector. High weighted terms in this vector represent the words most characteristic of that user’s preferences within the topic.\n",
    "\n",
    "The resulting set of topic-specific TF-IDF vectors constitutes the user’s content-based profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c17556fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top words for User1:\n",
      "\n",
      "dark:\n",
      "fight, blood, come, fall, wall, like, know, tell, stand, na, hear, gon, peopl, start, steadi, follow, riot, home, kill, yeah\n",
      "\n",
      "sadness:\n",
      "regret, greater, think, leav, place, want, blame, hold, lord, word, chang, caus, mind, trust, space, away, dream, suitcas, consequ, loos\n",
      "\n",
      "personal:\n",
      "abus, zayn, youth, younger, young, yesterday, yellow, year, yeah, wrong, write, amaz, american, anderson, angel, answer, anybodi, anymor, apart, apolog\n",
      "\n",
      "lifestyle:\n",
      "sing, rhythm, song, like, feel, strong, radio, kingdom, wheel, girl, come, think, tower, dial, midnight, letter, loud, write, know, eye\n",
      "\n",
      "emotion:\n",
      "good, feel, touch, miss, feelin, want, luck, go, hold, know, look, light, na, lip, like, control, caus, right, babi, kiss\n",
      "\n",
      "Top words for User2:\n",
      "\n",
      "sadness:\n",
      "break, heart, wave, silenc, crash, fall, like, spin, go, fade, away, leav, na, scar, dark, learn, come, echo, flood, save\n",
      "\n",
      "emotion:\n",
      "lip, eas, fade, away, kiss, like, freez, dream, blow, burn, final, sink, pour, shine, hop, breath, rain, hand, forget, blue\n",
      "\n",
      "Top words for User3:\n",
      "\n",
      "dark:\n",
      "blood, fight, come, know, hear, fear, like, feel, na, time, live, look, light, gon, hand, rais, stand, away, night, need\n",
      "\n",
      "lifestyle:\n",
      "sing, come, want, blue, right, wait, sky, play, know, rhythm, feel, song, tune, like, think, listen, hear, need, music, light\n",
      "\n",
      "personal:\n",
      "life, live, na, world, wan, know, chang, yeah, reason, like, lord, time, thank, come, good, teach, grind, dream, go, day\n",
      "\n",
      "sadness:\n",
      "ach, z, youth, young, yesterday, yellow, year, yeah, wrong, write, wreck, applaus, archiv, arm, around, ask, asleep, attract, awak, away\n"
     ]
    }
   ],
   "source": [
    "# Function to build user profile based on liked songs\n",
    "def build_profile(df, user_keywords):\n",
    "    profile = {}\n",
    "    for topic, keyword in user_keywords.items():\n",
    "        liked_songs = df[\n",
    "            (df['pred_topic'] == topic) &\n",
    "            (df['lyrics']\n",
    "               .str.lower()\n",
    "               .apply(lambda t: any(word.lower() in t for word in keyword)))\n",
    "        ]\n",
    "        # Make record of all words of this topic\n",
    "        all_words_of_topic = df[df['pred_topic'] == topic]['content'].tolist()\n",
    "        if not all_words_of_topic:\n",
    "            continue\n",
    "        # fit tfidf on all songs predicted as this topic\n",
    "        vectorizer = TfidfVectorizer(token_pattern=chosen_token_pattern, min_df=2)\n",
    "        vectorizer.fit(all_words_of_topic)\n",
    "        # build one big document containing all of liked songs\n",
    "        all_liked_songs = \" \".join(liked_songs['content'].tolist())\n",
    "        profile_vector = vectorizer.transform([all_liked_songs])\n",
    "        profile[topic] = (vectorizer, profile_vector)\n",
    "    return profile\n",
    "\n",
    "# Load user preferences from files\n",
    "def load_user(file):\n",
    "    processed_user = {}\n",
    "    for line in open(file):\n",
    "        topic, words = line.strip().split('\\t')\n",
    "        processed_user[topic] = words.split()\n",
    "    return processed_user\n",
    "\n",
    "user1 = load_user('user1.tsv')\n",
    "user2 = load_user('user2.tsv')\n",
    "user3 = {\n",
    "    'dark': ['night','shadow','blood','fear','death'],\n",
    "    'lifestyle': ['relax','fun','city','moment','freedom'],\n",
    "    'personal': ['life','dream','heart','soul','goal'],\n",
    "    'sadness': ['unrelevant','happy','cheerful','unrelated','recommender']\n",
    "}\n",
    "\n",
    "# build profiles\n",
    "profile1 = build_profile(train, user1)\n",
    "profile2 = build_profile(train, user2)\n",
    "profile3 = build_profile(train, user3)\n",
    "\n",
    "# Function to print top 20 words in each topic of the user profile\n",
    "def print_top_words(profile, label):\n",
    "    print(f\"\\nTop words for {label}:\")\n",
    "    for topic, (tfidf, vector) in profile.items():\n",
    "        array = vector.toarray().ravel()\n",
    "        index = np.argsort(array)[::-1][:20]\n",
    "        words = np.array(tfidf.get_feature_names_out())[index]\n",
    "        print(f\"\\n{topic}:\")\n",
    "        print(\", \".join(words))\n",
    "\n",
    "print_top_words(profile1, 'User1')\n",
    "print_top_words(profile2, 'User2')\n",
    "print_top_words(profile3, 'User3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f9e21",
   "metadata": {},
   "source": [
    "#### 1.3 User Profile Inspection\n",
    "The top 20 terms in each topic profile generally align well with the intended themes. Despite the overall accuracy, there are some interesting trends and behaviours. \n",
    "- User 1: \n",
    "    - Dark: *fight*, *blood*, *follow*, and *kill*\n",
    "    - Personal: terms such as *life*, *youth*, and *dream* dominate the list, despite minor noise from artist names like *abus* and *zayn*. Overall, the profile captures words of strong connection to the theme\n",
    "- User 3:\n",
    "    - For User 3, its **sadness** keywords were constructed to have unrelevant and contradicting words in which the algorithm searched Weeks 1–3 training songs for those whose predicted topic was sadness and whose lyrics contained at least one of these keywords. As expected, no songs satisfied both conditions and the code instead fit the TF-IDF vectorizer on all training songs whose predicted topic was sadness\n",
    "    - The resulting top 20 words revealed high frequency sadness tokens like *youth*, *young*, *yesterday* which align with common lyrical themes of reflection and loss\n",
    "\n",
    "To further reduce noise, each `TfidfVectorizer()` was configured with `min_df=2`, eliminating tokens that appear in fewer than two songs. This filter removed single letter noises and rare tokens, improving profile accuracy without sacrificing meaningful terms.\n",
    "\n",
    "In conclusion the topic-specific TF-IDF profiles successfully highlight each user’s interests. Despite minor noises, the main themes are clearly represented, and the profile well built for future recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31877cab",
   "metadata": {},
   "source": [
    "### 2. Evaluating Performance with Chosen Metrics\n",
    "With the user profiles constructed, the next step was to generate recommendations and assess their quality.\n",
    "\n",
    "At the heart of the recommendation strategy is the **cosine similarity** metric. For each user, their profile is represented as a TF-IDF vector derived from the songs they previously liked. Cosine similarity was then calculated between this profile vector and the TF-IDF vector of every Week 4 song. Cosine similarity measures the angle between two high-dimensional vectors, so higher scores indicate greater content alignment with the user’s established preferences.\n",
    "\n",
    "Once all similarities are computed, the songs were sorted in descending order of similarity. The top N songs were designated as recommendations.\n",
    "\n",
    "To evaluate how well these recommendations align with actual user interests, we use two standard metrics in recommender systems: **Precision@N** and **Recall@N**, which one focuses on relevance, and the other focuses on coverage:\n",
    "\n",
    "Two standard metrics were employed to assess recommendation quality:\n",
    "- **Precision@N** measures the proportion of recommended songs that were actually liked by the user. A high precision means the top N list is highly relevant\n",
    "- **Recall@N** measures the proportion of all liked songs that were successfully captured in the top N. High recall signifies comprehensive coverage of the user’s interests\n",
    "\n",
    "Selecting **N** involves balancing relevance and coverage. A small N may yield high precision but low recall, and a large N can weaken relevance and overwhelm the user. To achieve a practical balance, **N = 10** was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4947db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    User  Precision@10  Recall@10\n",
      "0  User1           0.6   0.153846\n",
      "1  User2           0.1   0.125000\n",
      "2  User3           1.0   0.043668\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity between songs and user profiles\n",
    "records = []\n",
    "for user_label, profiles in [\n",
    "        ('User1', profile1),\n",
    "        ('User2', profile2),\n",
    "        ('User3', profile3)\n",
    "    ]:\n",
    "    for index, song in test.iterrows():\n",
    "        topic = song['pred_topic']\n",
    "        if topic not in profiles:\n",
    "            continue\n",
    "        tfidf_model, profile_vector = profiles[topic]\n",
    "        song_vector = tfidf_model.transform([song['content']])\n",
    "        similarity = cosine_similarity(song_vector, profile_vector)[0,0]\n",
    "        records.append((user_label, index, similarity))\n",
    "\n",
    "# Function to get top N recommendations for each user (preference to tutorial's method)\n",
    "def get_top_n_recommendations(records, n=10):\n",
    "    top_n = {}\n",
    "\n",
    "    for uid, iid, score in records:\n",
    "        top_n.setdefault(uid, []).append((iid, score))\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = [iid for iid, _ in user_ratings[:n]]\n",
    "    return top_n\n",
    "\n",
    "top_n = get_top_n_recommendations(records, n=10)\n",
    "\n",
    "# Function to get actual likes based on user profiles\n",
    "actual_likes = {}\n",
    "for user_label, keywords in [\n",
    "        ('User1', user1),\n",
    "        ('User2', user2),\n",
    "        ('User3', user3)\n",
    "    ]:\n",
    "    for topic, keyword in keywords.items():\n",
    "        # Select songs from the test set that match the predicted topic\n",
    "        # and contain at least one of the user's keywords in the lyrics\n",
    "        liked_songs = test[\n",
    "            (test['pred_topic'] == topic) &\n",
    "            (test['lyrics']\n",
    "            .str.lower()\n",
    "            .apply(lambda t: any(word.lower() in t for word in keyword)))\n",
    "        ]\n",
    "        actual_likes.setdefault(user_label, set()).update(liked_songs.index)\n",
    "\n",
    "# Function to calculate precision and recall at N\n",
    "def precision_recall_evaluation(top_n, actual_likes, n=10):\n",
    "    rows = []\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        true_ids = actual_likes.get(uid, set())\n",
    "        true_likes = len(set(user_ratings) & true_ids)\n",
    "        # The number of liked songs in the top N recommendations\n",
    "        precision = true_likes / n\n",
    "        # The number of liked songs in the actual likes\n",
    "        if true_ids:\n",
    "            recall = true_likes / len(true_ids)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        rows.append({'User': uid, 'Precision@10': precision, 'Recall@10': recall})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(precision_recall_evaluation(top_n, actual_likes, n=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a84a58",
   "metadata": {},
   "source": [
    "#### 2.1 Evaluation of Final Performance\n",
    "The table above reports Precision@10 and Recall@10 for each user:\n",
    "- User 1: **Precision@10 = 0.60**, which means that 6 out of 10 recommended songs were liked. **Recall@10 = 0.1538**, meaning that approximately 15.4% of all liked songs appeared in the top-10 list. These results indicate a balanced trade-off between relevance and coverage.\n",
    "- User 2: **Precision@10 = 0.10, Recall@10 = 0.1250**. The low scores reflect the limited keyword set in this user’s profile, which reduced the system’s ability to identify relevant songs.\n",
    "- User 3: **Precision@10 = 1.00, Recall@10 = 0.0437**. This trend is interesting and means that all ten recommendations were liked, but they covered only a small fraction of the user’s full set of liked songs. This pattern might have been resulted from the sadness keyword assignment in Section 1.2, which forced the model to fall back on all songs predicted as sadness, yielding very precise but narrowly focused recommendations. In other words the model was confident in what a typical sadness song look like and therefore recommending extremely relevant songs. However, because the keywords did not match any of the actual lyrics, almost no test songs were considered truly liked.\n",
    "\n",
    "These outcomes demonstrate that recommendation quality focuses on profile specificity. Well-aligned profiles produce both high precision and reasonable recall, whereas vague or misleading profiles yield high precision but poor coverage. The cosine-similarity matching of TF-IDF profile vectors, evaluated through Precision@10 and Recall@10, provides a consistent and interpretable framework across diverse user scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eccb34",
   "metadata": {},
   "source": [
    "## Part 3 User Evaluation\n",
    "A user study was conducted to assess the content-based recommender developed in Part 2. The dataset was divided into four chronological weeks similar to Part 2:\n",
    "- Weeks 1–3 served as the training set\n",
    "- Week 4 used for evaluation of recommendations\n",
    "\n",
    "For each of Weeks 1–3, the subject was presented with **N = 10** randomly selected songs and asked to indicate which ones she liked. The liked songs from Weeks 1–3 were used to build the user profile through the **TF-IDF** and **cosine-similarity** pipeline described in Section 2. Finally, Week 4 recommendations, which are the top 10 by similarity, were generated and the subject again selected liked songs.\n",
    "\n",
    "The subject is a friend, who has no knowledge of recommendation algorithms, to ensure unbiased, perception-driven feedback. **Precision@10** and **Recall@10** were computed, with precision serving as the primary metric. This is because that with real user, the subject is only exposed to recommended songs generated from Week 4 and has no access to other sons. Therefore, she isn't able to reflect on how many other songs from the dataset she might have liked and the recall value will always be 1. Nonetheless, qualitative feedback on satisfaction, diversity, and alignment with preferences was also collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d37e3",
   "metadata": {},
   "source": [
    "### 1. Selecting Songs and Gathering User Feedbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "796dbf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 1 Songs:\n",
      "   index                         track_name        artist_name\n",
      "0    142                vivo hip hop (live)           skool 77\n",
      "1      6                          trap door         rebelution\n",
      "2     97                   outrunning karma      alec benjamin\n",
      "3     60  we are come to outlive our brains              phish\n",
      "4    112                 shout sister shout  madeleine peyroux\n",
      "5    181                    what i could do     janiva magness\n",
      "6    197                if you met me first      eric ethridge\n",
      "7    184                            natural    imagine dragons\n",
      "8      9                         never land     eli young band\n",
      "9    104                 john the revelator         larkin poe\n",
      "\n",
      "Week 2 Songs:\n",
      "   index                 track_name         artist_name\n",
      "0    392                stupid deep         jon bellion\n",
      "1    256                  black tar           the kills\n",
      "2    347            the good doctor               haken\n",
      "3    310              american tune     allen toussaint\n",
      "4    362           i have this hope  tenth avenue north\n",
      "5    431  heaven falls / fall on me            surfaces\n",
      "6    447        devils got you beat      blues saraceno\n",
      "7    434                 this is it   the wood brothers\n",
      "8    259                tesselation      mild high club\n",
      "9    354              snake charmer        parov stelar\n",
      "\n",
      "Week 3 Songs:\n",
      "   index                  track_name     artist_name\n",
      "0    643                    the fear       the score\n",
      "1    506           buy my own drinks    runaway june\n",
      "2    597                   get happy    goodbye june\n",
      "3    560            head above water   avril lavigne\n",
      "4    613                   tie me up        shenseea\n",
      "5    683       times won’t change me     circa waves\n",
      "6    699             ...a livication       iya terra\n",
      "7    686                      church    fall out boy\n",
      "8    509     you’re worthy of it all   shane & shane\n",
      "9    605  the dark horse always wins  blues saraceno\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and combine the coloumns into a single text column\n",
    "df = pd.read_csv(\"dataset.tsv\", sep='\\t')\n",
    "df = df.drop_duplicates().dropna()\n",
    "categories = ['artist_name', 'track_name', 'release_date', 'genre', 'lyrics']\n",
    "df['content'] = df[categories].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Define sample size and sample songs for three weeks\n",
    "N = 10\n",
    "week1 = df.iloc[0:250].sample(N, random_state=42)\n",
    "week2 = df.iloc[250:500].sample(N, random_state=42)\n",
    "week3 = df.iloc[500:750].sample(N, random_state=42)\n",
    "\n",
    "# Show user the sampled songs for Week 1-3\n",
    "print(\"Week 1 Songs:\")\n",
    "print(week1.reset_index()[['index', 'track_name', 'artist_name']])\n",
    "print(\"\\nWeek 2 Songs:\")\n",
    "print(week2.reset_index()[['index', 'track_name', 'artist_name']])\n",
    "print(\"\\nWeek 3 Songs:\")\n",
    "print(week3.reset_index()[['index', 'track_name', 'artist_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6643a04",
   "metadata": {},
   "source": [
    "After the user listening to the songs shown above, she chose the song she liked and marked down the index number of the song in following dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84071dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store user feedback\n",
    "user_liked_songs = {\n",
    "    'Week1': [97, 184],\n",
    "    'Week2': [310, 447, 259],\n",
    "    'Week3': [643, 699, 686]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7631f",
   "metadata": {},
   "source": [
    "### 2. Building User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad5dad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 track_name       artist_name\n",
      "881       boy in the bubble     alec benjamin\n",
      "823   the devil don't sleep  brantley gilbert\n",
      "942           whiskey fever           dorothy\n",
      "913     pressure under fire        gov't mule\n",
      "926              sit awhile   the band steele\n",
      "1003                 hayati         tinariwen\n",
      "799           do your worst        rival sons\n",
      "879                our town        tyler farr\n",
      "795         it's only right           wallows\n",
      "902              moon river      nicole henry\n"
     ]
    }
   ],
   "source": [
    "# Combine liked song from Weeks 1–3\n",
    "liked_song_combined = (\n",
    "    user_liked_songs['Week1'] +\n",
    "    user_liked_songs['Week2'] +\n",
    "    user_liked_songs['Week3']\n",
    ")\n",
    "liked_songs_df = df.loc[liked_song_combined].copy()\n",
    "\n",
    "# Preprocess the liked songs\n",
    "liked_songs_df['content'] = liked_songs_df['content'].apply(preprocess_text_recommender)\n",
    "train = df.iloc[:750].copy()\n",
    "train['content'] = train['content'].apply(preprocess_text_recommender)\n",
    "vectorizer = TfidfVectorizer(max_features=chosen_max_features, token_pattern=chosen_token_pattern)\n",
    "vectorizer.fit(train['content'])\n",
    "\n",
    "# Combine all liked song content into one document\n",
    "liked_text = ' '.join(liked_songs_df['content'].tolist())\n",
    "user_profile = vectorizer.transform([liked_text])\n",
    "\n",
    "# Preprocess the test set and calculate similarities\n",
    "test = df.iloc[750:1000].copy()\n",
    "test['content'] = test['content'].apply(preprocess_text_recommender)\n",
    "X_test  = vectorizer.transform(test['content'])\n",
    "similarities = cosine_similarity(X_test, user_profile).ravel()\n",
    "test['similarity'] = similarities\n",
    "\n",
    "# Get top 10 recommended songs\n",
    "top_n_recs = test.sort_values(by='similarity', ascending=False).head(10)\n",
    "print(top_n_recs[['track_name', 'artist_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b2be2",
   "metadata": {},
   "source": [
    "Then again, the songs above were shown to the user and her liked songs were marked and recorded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bb5d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "liked_songs_recommended = [881, 902, 1003, 799]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d048867",
   "metadata": {},
   "source": [
    "### 3. Evaluating Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0033212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.4\n",
      "Recall@10: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision and recall for the top 10 recommendations\n",
    "predicted_top10 = top_n_recs.index.tolist()\n",
    "actual_likes = set(liked_songs_recommended)\n",
    "true_likes = len(set(predicted_top10) & actual_likes)\n",
    "precision = true_likes / 10\n",
    "if actual_likes:\n",
    "    recall = true_likes / len(actual_likes)\n",
    "else:\n",
    "    recall = 0.0\n",
    "\n",
    "print(f\"Precision@10: {precision}\")\n",
    "print(f\"Recall@10: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f33a36",
   "metadata": {},
   "source": [
    "### 4. Conclusion and Real User Feedbacks\n",
    "The user evaluation yielded a **Precision@10 of 0.40**, indicating that 4 out of the 10 recommended songs were actually liked. This level of relevance is encouraging for a purely content-based system operating on a small data size. The **Recall@10 reached 1.0**, which was anticipated given that the user could only evaluate the ten songs presented. Unlike Part 2, where recall was computed against a broader set of liked songs using keyword matching, this live test had a constrained recommendation set, therfore inflating recall.\n",
    "\n",
    "Throughout the experiment, there were some interesting behaviors obeserved from the subject. During Week 1, the user instantly recognized “Outrunning Karma” by Alec Benjamin as which was her favorate artist and song. In Week 4, another Alec Benjamin track appeared among the top recommendations and was likewise liked. Although no explicit artist data was used, the TF-IDF representation might implicitly captured patterns associated with that artist, suggesting emergent personalization beyond topic matching.\n",
    "\n",
    "During an small interview of the subject, when asked whether lyrics alone would be enough for selection and no audio provided, the user expressed uncertainty. This underscores an important limitation on content-based recommendation system. Musical attributes like play a critical role in listener preference and are not captured in content-based models. So therefore, it is clear that in real user interview, the results could be biased or highly driven by external sources.\n",
    "\n",
    "Overall, the content-based recommender demonstrated strong precision given limited feedback, while the qualitative feedback highlighted both its strengths and its weaknesses. Future enhancements, such as integrating acoustic features, user collaborative signals, or even larger and more colorful datasets, could further improve recommendation quality and user satisfaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
